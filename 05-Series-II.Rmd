# Séries chronologiques II {#series2}

```{r setup, results='hide', warning=FALSE, include=FALSE}
SciViews::R
```

##### Objectifs {-}

- Pouvoir reconnaître les séries temporelles constituées d'addition de signaux élémentaires (modèle additif), ou de leur multiplication (modèle multiplicatif).

- Maîtriser les principales manières de filtrer un signal spatio-temporelle pour en extraire une caractéristique particulière.

- Être capable de décomposer une série régulière en ses différentes composantes, tendances, cycles et bruit blanc.

- Pouvoir régulariser une série irrégulière dans le but de l'analyser ensuite.

##### Prérequis {-}

Ce module nécessite que vous maîtrisiez parfaitement toutes les notions et techniques abordées dans le précédent module.


## Décomposition de séries

Toutes les techniques présentées dans cette partie ont pour but de décomposer des séries spatio-temporelles régulières (donc, à pas de temps constant et sans trous) en deux ou plusieurs composantes. On extrait soit une composante issue d’un filtrage ou d’un lissage, ou alors correspondant à un modèle (variation cyclique, tendance linéaire, polynomiale, exponentielle, etc, …). Quel que soit le nombre de composantes obtenues, la dernière est toujours le résidu (*residuals* en anglais), c’est-à-dire, la fraction de la série initiale (*series*) qui ne se retrouve dans aucune des autres composantes (*components*) extraites.

Pour un **modèle additif**, si nous extrayons par exemple deux composantes, on aura\ :

**residuals** = **series** – **component1** - **component2**

Un tel modèle est également appelé **linéaire** lorsque chaque composante est une combinaison linéaire de la ou des variables dépendantes, en l’occurrence pour les séries spatio-temporelles, le temps ou l’espace, ou les deux ensemble.

Un **modèle multiplicatif** se définira selon un schéma équivalent, mais dans ce cas, c’est le produit des composantes qui donnera la série. Une des composantes (la série filtrée, la série désaisonnalisée ou la tendance générale, selon le contexte), est alors exprimée dans les mêmes unités que la série de départ, alors que les autres sont des facteurs multiplicatifs adimensionnels. Il est possible de transformer un modèle multiplicatif en un modèle additif par une transformation logarithmiques du signal observé, puisque le *logarithme d’un produit* est égal à la *somme des logarithmes des facteurs*. Si l’opération donne une combinaison linéaire des logarithmes des facteurs, on dit que l’on a **linéarisé** le signal par la transformation logarithmique.

On pourrait encore définir des modèles **mixtes** où certaines composantes sont multiplicatives alors que d’autres sont additives, mais il n’y a alors plus moyen de les linéariser. Pour cette raison, de tels modèles sont très peu répandus. La nature de la ou des composantes dépend du traitement effectué. Par exemple pour un **filtrage**, il s’agit de la série filtrée, pour une **stationnarisation**, il s’agit de la tendance générale extraite, pour une **désaisonnalisation**, il s’agit du cycle saisonnier, etc.

Lorsqu'un cycle et une tendance à long terme sont tous deux présents dans la série, nous pouvons aisément discerner le modèle additif du modèle multiplicatif. En effet, dans le premier cas, l'**amplitude** du signal cyclique ne varie pas en fonction de la tendance, contrairement au second cas, comme illustré ci-dessous.

```{r, echo=FALSE}
time <- 1:300
trend1 <- 0.07 * time - 0.00035 * time^2 + 10
cycle1 <- 1 * sin(time/2) + 4
plot(time, trend1 + cycle1, type = "l", xlab = "Temps", ylab = "Série additive")
trend2 <- 0.07 * time - 0.00035 * time^2 + 10
cycle2 <- 0.5 * sin(time/2) + 1
plot(time, trend2 * cycle2, type = "l", xlab = "Temps", ylab = "Série multiplicative")
```

<!-- Ici, une ou deux apps shiny où on doit retrouver un signal complexe en mélangeant deux signaux simples parmi un liste de possibilités. Je pensais à un modèle cycle + tendance multiplicatif, et un autre où il faut mélanger deux cycles de périodes différentes. -->

##### A vous de jouer ! {-}

`r h5p(35, height = 270, toc = "Stationnarisation d'une série")`

### Fonction générale de décomposition

Dans {pastecs}, toutes les techniques de décomposition de série fonctionnent selon le même canevas et renvoient toutes un objet **tsd** (*time series decomposition*). Cet objet contient à la fois les composantes de la ou des séries qui ont été décomposées, des informations sur la méthode utilisée, ainsi que des données supplémentaires qui servent au diagnostic du traitement réalisé.

La fonction centrale qui effectue le traitement sur une ou plusieurs séries et renvoie un objet **tsd** est `tsd()`. Son argument `method=` permet de sélectionner une méthode de traitement à appliquer à toutes les séries à décomposer. Son argument `type=` indique si le modèle est additif ou multiplicatif. Cette fonction appelle d’autres fonctions plus simples `decxxx()` où `xxx` est le nom de la méthode (ex\ : `decmedian()` pour la méthode des médianes mobiles, par exemple, que nous étudierons plus loin). Les fonctions `decxxx()` ne peuvent traiter qu’une série à la fois, et ne sont normalement pas appelées directement par l'utilisateur^[Toutefois, étant donné que les fonctions `decxxx()` renvoient aussi des objets **tsd**, l’utilisateur avancé peut les préférer pour des questions de performance ou de simplicité dans ses propres scripts.].

Une fois qu’une décomposition est réalisée, il est possible d’extraire les différentes composantes (grâce à `tseries()`) ou une partie d’entre elles (en utilisant la méthode `extract()`) et de les retransformer en objet séries régulières (objet **ts** ou **mts**). On peut ainsi appliquer de nouvelles méthodes de décomposition sur un ou plusieurs composants et approfondir l’analyse des séries par des traitements de décomposition en cascade jusqu’à ce qu’on ait extrait et analysé toute l’information pertinente contenue dans ces séries.

##### A vous de jouer ! {-}

`r h5p(34, height = 270, toc = "But de la fonction tsd()")`

##### Exemple {-}

Notre jeu de données `co2` est un bon point de départ pour explorer la décomposition d'une série spatio-temporelle.

```{r}
data("co2", package = "datasets")
plot(co2)
```

En effet, nous voyons clairement deux composantes présentes\ : un cycle saisonnier d'une part, et une tendance à l'augmentation sur le long terme, d'autre part. De plus, le modèle est très clairement additif ici car l'amplitude du cycle ne dépend pas de la valeur atteinte par la tendance générale. Voyons en pratique comment se fait cette décomposition.

Une technique efficace pour lisser une série, et en particulier, en éliminer un cycle est la décomposition par moyennes mobiles. Pour l'instant, concentrez-vous sur la façon d'utiliser ce filtrage et sur ce qui en résulte. Nous aborderons un peu plus loin le détail des techniques employées. Nous utilisons donc `tsd(<series>, method = "average", type = "additive", <autres arguments>)`.

```{r, message=FALSE}
library(pastecs)
co2_dec <- tsd(co2, method = "average", type = "additive", order = 6, times = 3)
plot(co2_dec, col = 1:3)
```

Nous verrons plus loin ce que `order=` et `times=` veut dire. A noter que `type = "additive"` est la valeur par défaut et peut donc être omise ici. La méthode `plot()` est ensuite utilisée sur l'objet résultant (ici en utilisant les couleurs de la palette par défaut, 1 = noir, 2 = rouge et 3 = vert pour mettre en évidence les différentes séries). L'axe des abscisses numérote ici les observations par ordre croissant pour les retrouver facilement dans l'objet de départ si nécessaire. En haut sur le graphique, la série de départ en noir, au milieu en rouge, la série filtrée, et en bas en vert, les résidus qui contiennent entre autres le cycle.

L'objet `co2_dec` est de classe **tsd**.

```{r}
class(co2_dec)
```

Outre son graphique qui est des plus utiles pour juger le résultat de notre décomposition visuellement, nous pouvons retransformer toutes les composantes obtenues en un objet **mts** à l'aide de `tseries()`; ou alors, extraire une ou plusieurs composantes avec `extract()`. Effectuons les deux successivement. 

```{r}
co2_mts <- tseries(co2_dec)
class(co2_mts)
colnames(co2_mts) # Nom des séries dans l'objet mts
plot(co2_mts)
```
En partant d'un objet **ts** qui contient une série unique, nous aboutissons donc à un objet **mts** qui contient deux composantes nommées `filtered` et `residuals` après avoir appliqué `tsd(method = "average")` suivi de `tseries()`. La composante `filtered` est essentiellement la tendance à long terme dans le cas présent, tandis que les résidus sont constitués par la série **stationnarisée** dont la tendance générale est retirée. Nous pouvons parfaitement continuer à travailler avec cet objet **mts**, et c'est d'ailleurs assez pratique de conserver les deux composantes dans un même objet. Les analyses se font comme d'habitude en sélectionnant la série à traiter à l'aide de l'opérateur `[,]`\ :

```{r}
acf(co2_mts[, "filtered"])
```

Nous avons ici un parfait exemple de fonction d'autocorrelation pour une tendance pure.

Nous pouvons aussi **extraire** une ou plusieurs composantes de `co2_tsd`. Si nous souhaitons extraire la composante résiduelle uniquement sous forme d'un objet **ts** que nous appelerons `co2_stat`, nous ferons ceci\ :

```{r}
co2_stat <- extract(co2_dec, components = "residuals")
class(co2_stat) # C'est bien une série univariée
plot(co2_stat)
```

Comme `co2_stat` est maintenant une série stationnarisée, nous pouvons réaliser une analyse spectrale dessus.

```{r}
spectrum(co2_stat, span = c(5, 3))
```

Nous avons un cycle saisonnier très net, et un second pic mais tout juste pas significatif. Rien n'interdit de décomposer encore plus avant notre série, par exemple, en déterminant comment la composante saisonnière peut être ajustée ou non avec un signal sinusoïdal parfait (encore une fois, les détails de la technique seront abordés plus loin). Il nous suffit de construire un modèle sinusoïdal qui s'ajuste dans les données et d'utiliser ensuite `tsd(<series>, method = "reg", xreg = predict(<model>))` pour effectuer la décomposition de la série à l'aide de ce modèle (nous utilisons une astuce pour ajuster ce modèle sinusoïdal à l'aide de `lm()` que nous expliquerons, encore une fois, plus loin).

```{r}
t <- time(co2_stat) # t est la variable indépendante du modèle
co2_lm <- lm(co2_stat ~ I(cos(2*pi*t)) + I(sin(2*pi*t)))
plot(t, predict(co2_lm), type = "l")
```

L'objet `co2_lm` contient notre modèle. Sa méthode `predict()` permet de déterminer les valeurs de CO~2~ prédites par ce modèle, et c'est ces valeurs que nous utilisons ensuite pour notre décomposition\ :

```{r}
co2_dec2 <- tsd(co2_stat, method = "reg", xreg = predict(co2_lm))
plot(co2_dec2, col = 1:3)
```

Ici, nos deux composantes se nomment `model` et `residuals`. Que donne l'analyse spectrale sur les résidus après avoir éliminé notre cycle saisonnier parfaitement sinusoïdal\ ?

```{r}
co2_resid <- extract(co2_dec2, components = "residuals")
plot(co2_resid)
spectrum(co2_resid, span = c(7, 5))
```

La composante saisonnière a bien été éliminée, mais une composante de période de 2 ans apparaît maintenant significative. Nous pouvons continuer la décomposition, mais à ce stade, il faut rester prudent car les techniques de décomposition peuvent introduire des biais. Donc, nous devons nous demander si cette composante cyclique à deux ans est réelle ou s'il s'agit d'un artéfact des méthodes utilisées en amont. La réponse dépendra essentiellement des propriétés des techniques de décomposition... que nous allons étudier maintenant dans les sections suivante de ce module.

Les plus pointilleux d'entre vous auront remarqué un comportement bizarre de nos résidus à la seconde étape de décomposition, au début et en fin de série avec des valeurs qui deviennent très extrêmes. Ceci est un **effet de bord** lié à la façon dans les moyennes mobiles opèrent en début et fin de série. Cela ne se voit pas trop dans les résidus de la première décomposition, mais un effet d'amplification du biais apparaît lors de la seconde décomposition. **Vous devez toujours garder à l'esprit qu'aucune méthode de décomposition de série n'effectue un travail parfait. Des anomalies et des faux-positifs (par exemple, des "cycles fantômes") peuvent apparaître suite aux traitements réalisés antérieurement**. Ici, nous pouvons utiliser la fonction `window()` et raccourcir la série aux deux bouts pour éliminer l'artéfact. Cela n'aura pas trop de conséquences néfastes. Mais restez toujours vigilant\ ! Pour cette raison, et parce les séries combinant une tendance générale et un cycle saisonnier sont très fréquentes, les statisticiens ont mis au point des techniques qui permettent de séparer ces deux composantes des résidus *sans* artéfacts. L'une de ces techniques se nomme LOESS (prononcez "LO-ESSE", pas "LEUS"). Voici ce que cela donne sur `co2` (encore une fois, l'explication des arguments de la fonction sera réalisée plus loin).

```{r}
co2_loess <- tsd(co2, method = "loess", s.window = 6, trend = TRUE)
plot(co2_loess, col = 1:4)
```

Contrairement à la décomposition précédente, notez que nous avons laissé plus de libertés au signal saisonnier en vert qui varie donc légèrement d'une année à l'autre. Nous aurions aussi pu imposer le même signal (mais pas *nécessairement* sinusoïdal) chaque année en indiquant `s.window = "periodic"`. Naturellement ici, si les caractéristiques des composantes `trend` et `seasonal` sont assez évidentes, nous pourrions nous poser des questions concernant `residuals`. Comme cette série est stationnaire, nous pouvons directement en faire une analyse spectrale pour rechercher un ou plusieurs autres cycles de fréquence différente de un an.

```{r}
co2_resid2 <- extract(co2_loess, components = "residuals")
spectrum(co2_resid2, spans = c(5, 3))
```

Ici, tout se passe comme si nous avions éliminé totalement tout signal de fréquence entière (1, 2, 3, ...) et qu'ensuite d'autres signaux apparaissent. Ce sont probablement des cycles fantômes issus du traitement... tout comme le cycle à fréquence deux observé plus haut reste questionnable. **Gardez toujours votre esprit critique aiguisé en pareil situation\ !** Par exemple, quelle est l'étendue des signaux correspondants à nos trois composantes\ ? Pour la tendance, nous allons de 320 à 370 ppm, soit environ 50 ppm. Pour la composante saisonnière, l'amplitude du signal est de presque 3 ppm (le signal oscille entre 3 ppm et -3 ppm), soit une étendue de 6 ppm. Enfin, pour les résidus, l'échelle de l'axe s'étale de -0.6 à 0.4, soit 1 ppm, donc, une amplitude maximum de 0.5 ppm pour un éventuel cycle. Que peut-on encore extraire d'intéressant comme signal avec une si petite amplitude relative de presque un ordre de grandeur inférieur à l'amplitude du signal saisonnier qui lui, est bien concret dans notre série\ ? Voilà le type de raisonnement logique que vous devez utiliser, au delà de ce que les calculs vous montrent.

```{block2, type = 'note'}
Avec `tsd()` vous avez plusieurs méthodes de décomposition disponibles, même si nous n'en avons abordé jusqu'ici que deux. Avec `method=`\ :

- `"average"` pour les moyennes mobiles,
- `"median"` pour les médianes mobiles,
- `"diff"` pour la méthode des différence,
- `"evf"` pour le filtrage par les vecteurs propres de l'ACP,
- `"reg"` pour décomposer selon un modèle (droite ou courbe de régression),
- `"loess"` pour une décomposition plus complète en tendance générale, cycle saisonnier et résidus.
```


## Filtrage d’une série

Le filtrage d’une série consiste à remplacer chaque valeur de cette série par une combinaison de ses diverses valeurs. Lorsque le filtre conduit à réduire la variance en éliminant notamment les très hautes fréquences, on parle aussi de **lissage** du signal (*smoothing* en anglais). Nous allons à présent aborder quelques techniques de filtrage et découvrir dans quelles situations elles sont utiles.


### Moyennes mobiles

Le principe d'une **fenêtre mobile** consiste à balader une fenêtre de taille définie dans la série, centrée sur une observation $X_t$, et de remplacer cette observation par le résultat du calcul dans la fenêtre. Ensuite, la fenêtre coulisse d'une observation vers la droite et est donc centrée sur l'observation $X_{t+1}$. À nouveau, un calcul est appliqué sur les valeurs contenues dans le fenêtre, et le résultat du calcul remplace cette observation $X_{t+1}$, et ainsi de suite jusqu'à ce que toutes les observations de la série sont remplacés.

La taille de la fenêtre est définie par un paramètre *k* qui s'appelle l'**ordre** de la fenêtre mobile, correspondant au nombre d'observations comprises de part et d'autre de $X_t$ dans la fenêtre. Le nombre d'observations à l'intérieur de la fenêtre est donc de 2 *k* + 1 (*k* observations à gauche de $X_t$, $X_t$ lui-même et *k* observations à droite). Pour *k* = 2, chaque observation est donc remplacée par un calcul réalisé sur cinq observations successives.

Un problème se pose aux deux extrémités car la fenêtre dépasse de la série et des valeurs manquantes apparaissent. Il est possible de dupliquer les valeurs extrêmes pour remplir ces trous, de faire le calcul en éliminant ces valeurs manquantes, voire encore d'autres stratégies. Mais à tous les coups, ces artifices généreront des effets indésirables aux deux extrémités\ : les effets de bord. Il est aussi possible de raccourcir la série aux deux extrémités pour éluder la question tout simplement.

Dans le cas de la **moyenne mobile** (*moving average* en anglais), le calcul qui est réalisé à l'intérieur de la fenêtre est bien sûr la moyenne de ses observations. Pour reprendre le cas avec *k* = 2, si nous appelons $M_t$ la moyenne mobile calculée qui remplace $X_t$ dans la série filtrée, nous aurons\ :

$$M_t = \frac{X_{t-2} + X_{t-1} + X_t + X_{t+1} + X_{t+2}}{5}$$
et pour l'observation suivante\ :

$$M_{t+1} = \frac{X_{t-1} + X_{t} + X_{t+1} + X_{t+2} + X_{t+3}}{5}$$

et ainsi de suite... Cela se généralise bien sûr pour n'importe quelle valeur de *k*\ :

$$M_{t=i} = \sum_{j = -k}^k \frac{X_{t=i+j}}{2k + 1}$$

Nous avons donné **même poids** à chaque observation dans la fenêtre qui compte donc pour 1/5 chacune dans le cas *k* = 2. Dans ce cas, nous parlerons d'une moyenne mobile simple. Il est cependant possible de faire varier également les pondérations à l'intérieur de la fenêtre, mais nous n'utiliserons pas cette dernière variance ici.

#### Propriétés

Le filtrage effectue un *lissage* de la série, et la **bande de lissage** est égale à la largeur de la fenêtre, donc 2 *k* + 1. Cela signifie que cette technique de filtrage a la propriété d'**éliminer les signaux cycliques de fréquence égale à la bande de lissage**. Par exemple, pour des données mensuelles, nous pouvons éliminer l'effet saisonnier (on parle de **désaisonnalisation**) en utilisant une moyenne mobile de taille de fenêtre proche. Comme la taille doit être impaire, on choisira *k* = 6, ce qui donne une bande de lissage de 13 mois, pas trop éloignée des 12 mois visés.

Pour un effet accru, il est possible d'effectuer successivement plusieurs lissages par moyenne mobile. C'est l'argument `times=` qui contrôle cela dans `decaverage()` et `tsd(method = "average")`. A noter enfin, que les cycles de fréquence égale à la bande de lissage ne sont pas les seuls à être éliminés. D'autres fréquences sont également affectées à des degrés divers. Par exemple pour une désaisonnalisation, les fréquences 6, 4, 3, 12/5 et 2 mois sont également impactées à des degrés divers. Comprenez ici que le filtre n'a pas un comportement parfait et que des artéfacts apparaissent, du moins si on se limite à considérer que la moyenne mobile n'élimine *que* le cycle de fréquence égale à la bande de lissage\ !

Un autre phénomène qu'il faut connaître est l'**effet Slusky-Yule**. L'application d'un filtrage par moyenne mobile engendre une oscillation artificielle dont la période dépend à la fois de l'ordre de la fenêtre du lissage et le l'autocorrélation présente dans la série. Cette période peut se calculer (nous vous renvoyons au manuel de {pastecs} pour les détails de ce calcul).


##### A vous de jouer ! {-}

`r h5p(33, height = 270, toc = "But de la méthode des moyennes mobiles")`

##### Exemple {-}

Nous avons déjà réalisé une désaisonnalisation sur `co2`précédemment dans ce module. Voyons à présent comment ce filtre se comporte sur un signal cyclique complexe comme `lynx`.

```{r}
data("lynx", package = "datasets")
plot(lynx)
```

Nous voyons clairement des pics tous les 10 ans... mais aussi des pics plus importants tous les 40 ans, suggérant la présent d'un second cycle supéerposé au premier. Commençons par lisser notre série avec une moyenne mobile d'ordre 5 pour cibler le cycle de 10 ans.

```{r}
library(pastecs)
lynx_mb <- tsd(lynx, method = "average", order = 5, times = 1)
plot(lynx_mb)
```

Le signal filtré est encore très chaotique, mais il se lisse très rapidement si nous répétons plusieurs fois le lissage. Voilà ce que cela donne déjà avec `times = 3`\ :

```{r}
lynx_mb <- tsd(lynx, method = "average", order = 5, times = 3)
plot(lynx_mb)
```

Ici, nous voyons clairement une séparation du cycle à 40 ans qui reste dans la composante filtrée, tandis que le cycle à 10 ans a été éliminé dans la composante `residuals`. Maintenant, si nous choisissons plutôt un ordre *k* = 20, nous allons cibler le cycle à 40 ans.

```{r}
lynx_mb <- tsd(lynx, method = "average", order = 20, times = 3)
plot(lynx_mb)
```

Mais ce faisant, nous avons *aussi* éliminé le cycle à 10 ans dans les résidus\ ! Rappelez-vous que la moyenne mobile filtre également des signaux de fréquences dont la bande de lissage est multiple, dont ici à 10 ans. Il nous reste alors une tendance générale qui diminue légèrement pour remonter ensuite.

##### A vous de jouer ! {-}

```{r, echo=FALSE, results='asis'}
assignation("C05Ga_tsd", 
  url = "https://github.com/BioDataScience-Course/C05Ga_tsd",
  course.urls = c(
    'S-BIOG-025' = "https://classroom.github.com/a/V8--eYkL"),
  toc = "Décomposition d'une série spatio-teporelle")
```

### Médianes mobiles

Comme vous l'avez certainement deviné, les médianes mobiles fonctionnent comme les moyennes mobiles en baladant une fenêtre de largeur fixe le long de la série, mais cette fois-ci en calculant à chaque fois la médiane. Alors que la moyenne mobile s'exprime sous forme d'une somme de termes, ce qui l'associe à un modèle linéaire (on parle donc de filtrage linéaire), les médianes mobiles représentent par contre un *lissage non linéaire*. Il est possible de montrer que la répétition de cette opération conduit finalement à stabiliser le signal filtré. Cette courbe est caractérisée par une série de paliers successifs. Ainsi, la technique est utile pour *segmenter* une série, ce qui se rapproche des sommes cumulées pour détecter des transitions brusques dans la série (mais ici au lieu de les détecter, nous les mettons en évidence via le filtrage).

##### A vous de jouer ! {-}

`r h5p(36, height = 270, toc = "But de la méthode des médianes mobiles")`

##### Exemple {-}

Appliquons la méthode des médianes mobiles sur le signal de fluorescence dans `marphy`, dans l'espoir de séparer les masses d'eaux à niveaux de fluorescence différents.

```{r}
fluo <- ts(read("marphy", package = "pastecs")$Fluorescence)
plot(fluo, xlab = "Transect Nice - Calvi (stations)")
```

Que donne une fenêtre d'ordre 5\ ? (ici, il faut essayer différentes valeurs, il n'y a pas de règle a priori).

```{r}
fluo_smooth <- tsd(fluo, method = "median", order = 5, times = 1)
plot(fluo_smooth, xlab = "Transect Nice - Calvi (stations)")
```

L'effet n'est pas flagrant ici. Répétons l'opération trois fois.

```{r}
fluo_smooth <- tsd(fluo, method = "median", order = 5, times = 3)
plot(fluo_smooth, xlab = "Transect Nice - Calvi (stations)")
```

... et puis dix fois.

```{r}
fluo_smooth <- tsd(fluo, method = "median", order = 5, times = 10)
plot(fluo_smooth, xlab = "Transect Nice - Calvi (stations)")
```

Le signal est quasiment identique entre `times=3` et `times=10`. Nous avons effectivement stabilisé la courbe lissée par l'application répétée des médianes mobiles. Nous voyons ici clairement apparaître le pic de fluorescence caractéristique d'une production en phytoplancton importante au niveau du front (remontée d'eau froides profondes, riches en nutriments).

Ce genre de lissage se représente mieux en superposant le signal de départ et la série lissée. Ceci est possible en indiquant `stack = FALSE` et `resid = FALSE` lors de l'appel de `plot()`\ :

```{r}
plot(fluo_smooth, stack = FALSE, resid = FALSE, col = 1:2,
  xlab = "Transect Nice - Calvi (stations)")
```

Les pics sont bien ici remplacés par des paliers.

### Filtrage par différences

La méthode des différences a pour but d’éliminer la tendance. Ce n’est valable que si la série a une tendance monotone et non en "dents de scie". Autrement dit, cela suppose que la série est autocorrélée positivement ou négativement.

Nous allons ici récupérer l'opérateur retard $LX_t$ vu tout au début du module 4. Cet opérateur décale la série d'une ou plusieurs observations (argument `lag=`) par rapport à la série d'origine. La méthode des différences consiste à **soustraire** la série ainsi décalée par rapport à la série de départ. Naturellement, le filtrage peut être répété plusieurs fois pour en amplifier l'effet. Ce filtrage est également *linéaire*.

A condition de ne pas avoir également un cycle important, le filtrage par différences est efficace pour **éliminer une tendance monotone croissante ou décroissante**, voire une tendance de forme quelconque, mais dont la courbure varie lentement (signal à très haute fréquence). Dans ce cas, la méthode est très efficace pour **stationnariser** une série.

##### A vous de jouer ! {-}

`r h5p(32, height = 270, toc = "But de la méthode des différences")`

##### Exemple {-}

Toujours pour le signal de fluorescence dans `marphy`, nous pouvons éliminer la tendance générale comme ceci (les valeurs optimales pour `lag=` et `times=` sont à trouver de manière empirique ici)\ :

```{r}
fluo_stat <- tsd(fluo, method = "diff", lag = 2, times = 1)
plot(fluo_stat, xlab = "Transect Nice - Calvi (stations)")
```

Malheureusement, en présence de cycles importants, la méthode n'est pas très efficace, même si le décalage est choisi pour coïncider avec le cycle. Ainsi pour `co2`, nous aurons\ :

```{r}
data("co2", package = "datasets")
co2_stat <- tsd(co2, method = "diff", lag = 12, times = 1)
plot(co2_stat)
```

Comme nous pouvons le voir, la tendance a bien été éliminée, mais malheureusement, le cycle saisonnier est également passé à la trappe, et ce malgré notre précaution de faire coïncider le décalage (`lag=`) avec les fréquence du cycle.

### Filtrage par les valeurs propres

Le filtrage par les valeurs propres (*eigenvector filtering* en anglais, ou EVF en abrégé) consiste à décomposer le signal original de la série via une analyse en composantes principales.

## Régressions et séries

Les régressions linéaires et non linéaires, surtout s'il s'agit de modèles ajustés par les moindres carrés, ne sont *a priori* pas compatibles avec les séries spatio-temporelles puisqu'elles nécessitent une indépendance des observation, une distribution Normale des résidus, etc. Mais en fait toutes ces conditions s'appliquent aux **tests d'hypothèses** autour de le régression. Si nous nous contentons d'utiliser la régression pour estimer la forme d'une composante, et ensuite que nous l'extrayons sans utiliser le tableau de l'ANOVA, les tests de Student sur les paramètres, etc., alors notre usage de cet outil sera approprié ici.

### Estimation de la tendance par régression

Les méthodes de filtrage présentées ci-dessus permettent d’extraire ou d’éliminer une tendance de forme quelconque (on n’a pas d’idée *a priori* de sa forme\ ; on ne cherche pas à la modéliser). Les méthodes d’estimation par régression, au contraire, font appel à des hypothèses très fortes quant à sa nature\ : on cherche à ajuster une ou plusieurs équations fonctionnelles à la tendance de la série.

L’idée simple pour estimer une tendance générale est de vérifier son ajustement par une droite, une parabole, un polynôme d’ordre plus élevé simulant un mouvement pseudo-cyclique de forte période. Ces techniques reposent sur l’algorithme des moindres carrés\ : on minimise les carrés d’écarts entre les données observées et un polynôme de degré fixé à l’avance. 

On pourrait imaginer bien d’autres formes de tendance générale (exponentielle, logistique, etc.). Certains de ces modèles prennent même en compte une autocorrélation entre les observations, aspect fondamental dans le cadre de l’analyse de séries spatio-temporelles. Ces modèles autorégressifs sont très importants dans le domaine, mais ils sont plutôt appliqués en économie, en physique, ... et moins en biologie. Nous ne les étudierons donc pas ici.

La tendance générale est parfois liée à l’évidence à un cycle (annuel, lunaire, etc.). La régression sinusoïdale permet de tester statistiquement, et de définir un modèle de variation rigoureusement périodique. Naturellement, ce modèle doit être utilisé avec discernement. Ainsi, même si la variabilité saisonnière existe pour des abondances d’espèces, elle ne s’ajuste pas forcément à une sinusoïde si les amplitudes maximums sont décalées d’une année à l’autre, d’un mois ou plus. Et si alors on étudie les écarts entre les valeurs observés et le modèle, ceux-ci ne correspondent plus à une variabilité indépendante de la variation saisonnière. Ils ne représentent que l’inadéquation du modèle choisi, qui lui, est strictement périodique.

Grâce à une astuce, nous pouvons ajuster un modèle sinusoïdal à l'aide de la fonction `lm()` dans R. Il suffit de rendre l'équation linéaire par un changement de variables. Rappelez-vous que nous avons vus dans le précédent module qu'une sinusoïde correspond à l'équation suivante\ :

$$X_t = A \cdot \cos(2 \pi \omega t + \phi)$$

avec $\omega$ la fréquence du signal, $\phi$ son déphasage et $A$ son amplitude. Mais nous avons aussi vu qu'il est possible de reparamétrer cette forme en une somme de sinus et cosinus\ :

$$A \cdot \cos(2 \pi \omega t + \phi) = \beta_1 \cdot \cos(2 \pi \omega t) + \beta_2 \cdot \sin(2 \pi \omega t)$$

Dès lors, et à condition de cibler une fréquence particulière $\omega$ connue à l'avance pour le cycle (par exemple, pour un cycle saisonnier on sait d'avance que la fréquence est d'exactement un an), nous pouvons calculer deux variables issues de $t$\ : $T_1 = \cos(2 \pi \omega t)$ et $T_2 = \sin(2 \pi \omega t)$. Ainsi, nous obtenons l'équation suivante\ :

$$\beta_1 \cdot T_1 + \beta_2 \cdot T_2$$

Cette dernière forme est assimilable à un polygone d'ordre deux dont l'ordonnée à l'origine vaut zéro, et que nous pouvons ajuster par régression polynomiale à l'aide de la fonction `lm()`.

##### Exemples {-}

### Tendance linéaire

On peut effectuer une régression linéaire sur la série ***Density*** du jeu de données ***marphy*** afin de l'utiliser comme estimation de la tendance générale dans une décomposition de cette série spatio-temporelle. La variable indépendante (x) est le temps que l'on reconstitue à partir de la série spatio-temporelle à l'aide de la fonction ***time()***. Ensuite, on effectue la régression linéaire à l'aide de la fonction ***lm()******* qui demande de spécifier le modèle sous forme **Y ~ X**, avec **Y** étant la variable dépendante et **X** étant la variable indépendante. La méthode ***summary()******* retourne un rapport détaillé sur cette régression:

&gt; data(marphy)
&gt; density &lt;- ts(marphy\[, "Density"\])
&gt; Time &lt;- time(density)
&gt; density.lin &lt;- lm(density ~ Time)
&gt; summary(density.lin)

Call:
lm(formula = density ~ Time)

Residuals:
 Min 1Q Median 3Q Max
-0.052226 -0.018014 0.001945 0.017672 0.058895

Coefficients:
 Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) 2.884e+01 6.585e-03 4379.27 &lt;2e-16 \*\*\*
Time 3.605e-03 1.659e-04 21.73 &lt;2e-16 \*\*\*
---
Signif. codes: 0 \`\*\*\*' 0.001 \`\*\*' 0.01 \`\*' 0.05 \`.' 0.1 \` ' 1

Residual standard error: 0.02685 on 66 degrees of freedom
Multiple R-Squared: 0.8774, Adjusted R-squared: 0.8755
F-statistic: 472.3 on 1 and 66 DF, p-value: 0

 Notons que le tableau de l'ANOVA indique qu'aussi bien la pente **Time** que l'ordonnée à l'origine **(Intercept)** sont très hautement significatifs. Mais rappelons que nous violons ici une hypothèse de base de la régression linéaire puisque les termes d'erreurs présentent très probablement une certaine autocorrélation et ne sont donc pas totalement indépendants entre eux! On peut cependant considérer les résultats de l'ANOVA comme indicatifs, tout en gardant à l'esprit qu'ils sont biaisés. Les valeurs de densité correspondant à ce modèle sont récupérées à l'aide de la méthode ***predict()*******, et nous pouvons maintenant superposer le modèle au graphe de la série originale.

&gt; xreg &lt;- predict(density.lin)
&gt; plot(density)
&gt; lines(xreg, col=3)
&gt; title("Linear model for trend in 'density'")

La décomposition de la série ***density*** est réalisée grâce à ***decreg()******* comme suit. Nous obtenons un objet *'tsd'* qui nous est maintenant familier, ainsi que le type de représentation graphique à l'aide de la méthode ***plot() ***qui lui est associé:

&gt; density.dec &lt;- decreg(density, xreg)
&gt; plot(density.dec, col=c(1, 3, 2), xlab="stations")

### Tendance polynomiale

Nous pouvons bien entendu nous demander si une droite est le meilleur modèle qui soit pour représenter la tendance générale dans cette série. Ainsi, par exemple, on aurait pu également choisir un polynôme d'ordre 2 pour représenter cette tendance, ce qui donne\ :



C'est effectivement beaucoup mieux que le modèle linéaire. La décomposition de la série à l'aide de ***decreg()******* se fait comme précédemment:

&gt; density.dec2 &lt;- decreg(density, xreg2)
&gt; plot(density.dec2, col=c(1, 3, 2), xlab="stations")

### Tendance non linéaire

Nous pourrions bien entendu tester également des polynômes d'ordre 3, 4,…, mais nous nous arrêterons-là dans notre exemple pour tester une autres possibilité\ : l'utilisation d'un modèle non linéaire pour représenter la tendance générale dans cette série, par exemple, une courbe logistique\ :

```{r}
#dens_logis <- nls(density ~ SSlogis(Time, Asym, xmid, scal))
#summary(dens_logis)
```

Nous utilisons maintenant la fonction ***nls()******* en lieu et place de la fonction ***lm()***. La définition du modèle est quelque peu différente: **Y ~ model(X, params,…)**, avec **model()**, une fonction non linéaire. Lorsque la fonction est capable de calculer elle-même ses paramètres de départ, ces derniers ne doivent pas êtres fournis et l'utilisation de la fonction ***nls()*** est alors presque aussi simple que celle de ***lm()***, mais il n'en va pas toujours ainsi. Voyez l'aide en ligne de ***nls()*** pour plus de renseignements sur l'utilisation de la régression non linéaire dans S+ ou R. Nous observons que, ici encore tous les paramètres semblent très hautement significatifs dans le modèle. Nous ne commenterons pas plus loin les résultats renvoyés par la méthode *** summary()*******. L'extraction des valeurs prédites par le modèle, le graphe superposé de la fonction de départ et la décomposition de la série s'obtiennent comme avec un modèle linéaire:

&gt; xregl &lt;- predict(density.logis)
&gt; plot(density)
&gt; lines(xregl, col=3)
&gt; title("Logistic model for trend in 'density'")

&gt; density.decl &lt;- decreg(density, xregl)
&gt; plot(density.decl, col=c(1, 3, 2), xlab="stations")

###### Modèle sinusoïdal

Nous avons déjà ajusté un tel modèle dans le jeu de données `co2` plus haut, mais sans l'expliquer complètement. A présent que nous avons vu l'astuce de la substitution de variable dans la forme sinus/cosinus de l'équation de la courbe sinusoïdale, nous comprenons mieux comment nous avons procédé.

TODO: reprendre l'exemple ici et l'expliciter.

## Décomposition par LOESS

LOESS est une méthode de régression polynomiale locale. Pour chaque valeur *X*<sub>*t*</sub> d’une série *X*, on va considérer les *k* voisins à gauche et à droite, éventuellement pondérés (par exemple par la distance les séparant de *X*<sub>*t*</sub>). On effectue une régression par les moindres carrés d’un polynôme d’ordre *p* (habituellement *p* vaut 1 ou 2), et on récupère la valeur prédite au temps *t* (voir Cleveland *et al*, 1992). Si l’ordre *p* du polynôme vaut 0, on se ramène à une moyenne mobile.

Avec un choix judicieux de *k*, et éventuellement en réitérant le processus, on va pouvoir extraire la tendance générale. Comme avec la méthode des moyennes mobiles, on va aussi pouvoir désaisonnaliser une série en prenant une fenêtre égale à un an, donc, en considérant 12 termes pour des données mensuelles (13 en fait pour une fenêtre centrée sur les observations de la série). Une combinaison de ces deux traitements va permettre de décomposer une série en tendance, cycle saisonnier et résidus selon un modèle additif. On peut aussi n’extraire que le cycle saisonnier et les résidus (surtout utile lorsque la tendance générale *est* périodique sur un an).

Comme les résidus ont rarement une distribution normale et ne sont souvent pas indépendants entre eux dans les séries, les hypothèses de base indispensables pour la régression à l’aide de la méthode des moindres carrés sont violées la plupart du temps. Ainsi, pour obtenir une estimation plus exacte des composantes, il est possible d’utiliser une méthode de régression plus robuste (*i.e.*, moins sensible à la violation de ces hypothèses de base). On peut par exemple décider de pondérer les valeurs de manière inversement proportionnelle à leur influence respective sur la régression. Ainsi, une valeur qui, à elle seule, aurait tendance à tirer la courbe vers elle (valeur isolée, très éloignée du nuage formé par toutes les autres) recevra une pondération très faible afin de minimiser son effet.

##### Exemple {-}

L’utilisation de ***decloess()******* pour extraire un cycle saisonnier à partir de la méthode des différences a déjà été présentée dans l’exemple de ***tsd()***. Nous le reprenons ici, et nous explorerons ensuite d’autres valeurs pour les paramètres, afin de voir leurs effets sur la décomposition:

&gt; data(releve)
&gt; melo.regy &lt;- regul(releve$Day, releve$Melosul, xmin=9, n=87,
+ units="daystoyears", frequency=24, tol=2.2, methods="linear",
+ datemin="21/03/1989", dateformat="d/m/Y")
&gt; melo.ts &lt;- tseries(melo.regy)
&gt; melo.dec &lt;- decloess(melo.ts, s.window="periodic")
&gt; plot(melo.dec, col=1:3)

Avec l’option **s.window = "periodic"**, la forme de la composante cyclique saisonnière est quelconque, mais est reproduite à l’identique pour toutes les années (puisque la même moyenne mensuelle est utilisée pour chaque mois de chaque année). Avec l’option **s.window = 13** et **s.degree = 0**, on obtient pratiquement le même résultat. Par contre, avec **s.degree = 1**, on va permettre une variation de la composante cyclique saisonnière d’une année sur l’autre:

&gt; melo.dec2 &lt;- decloess(melo.ts, s.window=13, s.degree=1)
&gt; plot(melo.dec2, col=1:3)

Une étude de la cohérence (voir analyse des séries spatio-temporelle, paragraphe analyse spectrale croisée) entre les deux composantes ***deseasoned*** et ***seasonal*** sous R donne:

&gt; melo.tsd &lt;- tseries(melo.dec2)
&gt; melo.spc &lt;- spectrum(melo.tsd, spans=c(3,3), plot=FALSE)
&gt; plot(melo.spc, plot.type="c")

On peut voir que la cohérence est très faible entre les deux séries, c’est le principe même de la décomposition saisonnière. Enfin, sous R uniquement, on peut également extraire la tendance (selon un modèle additif), simultanément à un cycle saisonnier qui se répète d’année en année (**s.window = "periodic"**) ou qui évolue au cours du temps (**s.window  frequency**), en précisant **trend = TRUE**:

&gt; melo.dec3 &lt;- decloess(melo.ts, s.window="periodic", trend=TRUE)
&gt; plot(melo.dec3, col=c(1,4,3,2))

##### A vous de jouer ! {-}

```{r, echo=FALSE, results='asis'}
assignation("C05Gb_ts_adv", 
  url = "https://github.com/BioDataScience-Course/C05Gb_ts_adv",
  course.urls = c(
    'S-BIOG-025' = "https://classroom.github.com/g/ZD6gTPvE"),
  toc = "Analyse libre d'une série spatio-teporelle")
```

## Régularisation 

Les méthodes présentées dans cette partie permettent de transformer une série spatio-temporelle échantillonnée avec un pas de temps irrégulier ou présentant des trous, en une série régulière. Malheureusement, en biologie, on a rarement affaire à des séries parfaitement régulières, soit parce qu’il y a des observations manquantes, soit parce qu’on ne contrôle pas l’intervalle de temps. Dans ce cas, la régularisation est une opération préliminaire indispensable à beaucoup d’analyses de séries spatio-temporelles, que ce soit avec les outils {pastecs} ou avec d'autres méthodes. Les objets **ts** doivent être *d’office* des séries spatio-temporelles régulières dans R. Cette étape est donc requise pour adapter des **data frames** avant de pouvoir les transformer en séries temporelles\ ! Plusieurs méthodes de régularisation sont disponibles dans {pastecs}\ : régularisation par valeur constante (`regconst()`), linéaire (`reglin()`), par courbes splines (`regspline()`) et par la méthode des aires (`regarea()`). Afin de simplifier le travail de régularisation, et de faciliter ensuite la création des séries temporelles, toutes ces fonctions ont été regroupées sous une fonction commune `regul()`.

```{block2, type = 'warning'}
Toutes ces méthodes sont à utiliser **avec précaution**. Il est très facile d’obtenir n’importe quoi si l’on choisit des mauvais paramètres\ ! Il n’y a pas de règle absolue dans leurs choix, étant donné qu’ils sont liés à la nature-même des données à régulariser. Par exemple, il faut être extrêmement attentif à ne pas régulariser à tort des données physico-chimiques (par exemple à l’aide de la méthode des aires avec une très grande fenêtre).
```

Quelle que soit la méthode utilisée, on doit veiller à obtenir un pas de temps de la série régulière aussi proche que possible du pas de temps (moyen) de la série échantillonnée. De même, on doit aussi essayer d’obtenir autant de valeurs non interpolées que possible dans la série régulière finale. Quant à l’extrapolation (obtenue à l’aide de l’argument `rule = 2`), elle est à proscrire. La fonction `regul()` admet beaucoup de paramètres. Vous êtes invité à essayer différents réglages sur ses propres données, et à visualiser le résultat à l’aide de `plot.regul()` et `hist.regul()` pour vous familiariser avec ses possibilités.

Dans le cas où l’on voudrait condenser des données aléatoires (par exemple, transformer un échantillonnage mensuel en valeurs saisonnières ou des données quotidiennes en série par semaine ou par mois), il vaut mieux condenser les données en calculant des moyennes ou des médianes successives. On peut aussi régulariser la série avec le pas temps initial si ce dernier est un multiple du pas de temps visé, et utiliser ensuite la fonction `aggregate()` de R, que nous avons déjà utilisée.

Dans le cadre de ce cours, la régularisation de série irrégulière est une matière optionnelle. Cependant, comme il y a de fortes chances que vous rencontriez des séries irrégulières en biologie, 

### Choix du pas de temps

Dans le cas de séries initiales très irrégulières, les fonctions `regul.screen()` et `regul.adj()` permettent de déterminer quels sont les meilleurs paramètres temporels pour la série régularisée afin qu’un maximum d’observations de la série initiale coïncident avec les valeurs de temps échantillonnées pour la série régularisée. Une fenêtre de tolérance pour déterminer si les valeurs coïncident est utilisée. Celle-ci est calculée en interne à l’aide de la fonction `match.tol()` qui ne sera donc en principe pas utilisée directement. Un exemple d'utilisation de ces fonctions est donné dans le paragraphe suivant.

### Fonction de régularisation

Dans {pastecs}, la régularisation d’une ou plusieurs séries se fait grâce à la fonction `regul()` qui renvoie un objet **regul** (en fait, une liste contenant un **data frame** qui représente la ou les séries régularisées, accompagnées d’un certains nombres d’attributs supplémentaires qui permettent le diagnostic du traitement, ainsi que l’extraction aisée des objets time series **ts** à partir de cette matrice à l’aide de `extract()` ou `tseries()`. La méthode `summary()` résume le résultat de la régularisation. La méthode `specs()` récupère les spécifications d’un objet **regul** pour éventuellement les appliquer à une nouvelle régularisation. La méthode `plot()` permet de réaliser facilement le graphe superposé de la série initiale et de la série finale à des fins de comparaison. La méthode `lines()` permet de superposer à un graphe déjà tracé une autre série régularisée (par exemple pour comparer deux méthodes de régularisation entre elles). La méthode `identify()` permet de sélectionner des points sensibles lors de la régularisation directement sur le graphe. La méthode `hist()` permet de représenter les points qui correspondent entre la série originale et la série régularisée, en fonction de la tolérance choisie dans le temps pour déclarer que les données sont équivalentes. Pour l’utilisateur avancé, les fonctions de régularisations respectives `regconst()`, `reglin()`, `regspline()` et `regarea()` sont aussi disponibles mais ne prennent pas en charge tout le traitement annexe de diagnostic et d’extraction de séries chronologiques. Par contre, ces dernières fonctions peuvent s’avérer plus pratiques dans des scripts et programmes personnels.

Dans la plupart des logiciels, le temps est exprimé par rapport à une date de départ arbitraire (par exemple, le 1^er^ janvier 1970 par défaut dans R). Les dates antérieures à cette valeur de départ sont exprimées par des valeurs négatives. Les dates sont manipulées en interne comme des nombres décimaux classiques. La partie entière de la date représente le jour, la partie décimale représente l’heure. Par exemple, le nombre 1.5 représente le 2 janvier 1970, à 12:00:00 (le "second jour et demi" par rapport à la date de départ qui vaut 0) dans R, si l'origine temporelle n'a pas été redéfinie. Il est possible de connaître l'origine temporelle utilisée sur votre système grâce à la function `getOption()`, et la modifier en appelant la même fonction\ :

```{r}
options("chron.origin") # Regarde sa valeur
options(chron.origin = c(month = 1, day = 1, year = 1990))
options("chron.origin")
options(chron.origin = NULL) # Revient vers la valeur pas défaut
options("chron.origin")
```

Lorsque `chron.origin = NULL`, cela signifie que c'est la valeur par défaut du 01/01/1970 dans R qui est prise en compte.

```{block2, type = 'warning'}
Si vous définissez des dates, et puis que vous modifiez `chron.origin` entre deux calculs, les mêmes variables risquent de se référer à des dates différentes avant et après le changement! Pour éviter cela, et aussi pour prendre en compte les fuseaux horaires et les horaires été/hiver, R utilise un format plus complexe de dates appelé **POSIXt**. Nous invitons le lecteur intéressé à consulter l'aide en ligne sous R à la rubrique `?DateTimeClasses`.
```

Les séries temporelles dans R admettent toute autre représentation arbitraire du temps en mode décimal. On peut par exemple décider de décompter les secondes écoulées à partir d’un temps de référence à l’aide de la partie entière du nombre (dixièmes, centièmes, etc. de secondes pour la partie décimale). Cela pourra être renseigné par l’argument `units=` de `regul()` dans lequel on entrera `"sec"` au lieu de `"days"` par défaut. L'utilisateur peut donc se référer à n'importe quelle échelle de temps, avec une unité au choix qu'il peut préciser dans l'argument `units=`. Cependant, il est conseillé de s'en tenir aux unités reconnues par les fonctions de la libraire PASTECS, parce que cela permettra notamment un formattage idéal des unités dans les sorties graphiques à l'aide de la fonction `GetUnitText()` (utilisée en interne, et donc, normalement pas appelée directement par l'utilisateur). Ces unités reconnues sont\ : `"years"`, `"days"`, `"weeks"`, `"hours"`, `"min"` et `"sec"`.

Une échelle particulièrement utile pour manipuler des séries pluriannuelles est **"years"**. Dans ce mode, la partie entière représente l’année, et la partie décimale représente le mois, le jour, l’heure, etc… Ainsi par exemple, 1998.167 représente le 1<sup>er</sup> mars 1998 à 00:00:00 heures, soit 1998 + 2/12 (0/12 pour début janvier, 1/12 pour début février, 2/12 pour début mars, …, 11/12 pour début décembre). En général, on travaille sur des données mensuelles dans ce mode, et on arrondit au mois près. Il s’agit en réalité d’une approximation puisque tous les mois n’ont pas le même nombre de jours (ni toutes les années), mais c’est une représentation pratique qui permet de se débarrasser des tracas liés aux nombres de jours par mois, aux années bissextiles, etc… Dans la fonction *** regul()***** **il est possible de préciser **units = "daystoyears"** qui convertit automatiquement des dates classiques où l’unité est le jour dans le mode d’unité **"years"**.

> A noter que, d'une manière générale, l'analyse de cycles bien précis tels que l'effet saisonnier ou les cycles circadiens nécessitent d'exprimer le temps dans une unité qui correspond exactement au cycle étudié. Ainsi, les fonctions de décomposition des séries en composantes saisonnières (**decloess()** par exemple, voir chapitre décomposition des séries spatio-temporelles) imposent d'exprimer le temps en unité **"years"**. De même, il est fortement conseillé d'adopté l'unité **"days"** pour l'étude de cycles circadiens.

La fonction ***regul()*** offre encore une autre particularité. Elle permet de ne pas interpoler des données qui seraient présentes dans la série de départ, ***avec une fenêtre de tolérance******** réglable***. Ainsi par exemple, si une mesure est présente au jour 123, mais aucune au jour 124 dans la série initiale, et qu’il faut calculer une valeur dans la série régularisée au jour 124 (pas d’une semaine ou plus), on peut décider que la valeur échantillonnée au jour 123 est une très bonne estimation de la valeur au jour 124 et ne pas effectuer d’interpolation. La fenêtre dans laquelle il faut aller voir si une valeur mesurée existe est réglable par le paramètre **tol** de ***regul()***. De plus, on peut décider d’aller voir de chaque côté, uniquement à gauche, ou seulement à droite (voir paramètre **tol.type**). Ainsi, on peut accroître sensiblement le nombre de valeurs non interpolées dans la série régularisée, en considérant une certaine "souplesse" dans les dates entre les séries échantillonnées et régularisées. Si cela n'est pas simple, un exemple concret va clarifier les choses…

##### Exemple {-}

Chargeons le jeu de données ***releve*** à partir de la librairie PASTECS. Etant donné que cette série est très irrégulière comme on peut le voir ci-dessous, il est très difficile de décider quel est le meilleur pas de temps (**deltat**), le meilleur temps initial (**xmin**), ainsi que le nombre de mesures optimal (**n**) pour interpoler le moins de valeurs possibles:

&gt; data(releve)\# Dans R uniquement
&gt; releve$Day
\[1\] 1 51 108 163 176 206 248 315 339 356 389 449
\[13\] 480 493 501 508 522 554 568 597 613 624 639 676
\[25\] 697 723 751 786 814 842 863 877 891 906 922 940
\[37\] 954 971 983 999 1010 1027 1038 1054 1069 1081 1094 1110
\[49\] 1129 1143 1156 1173 1186 1207 1226 1235 1249 1271 1290 1314
\[61\] 1325
&gt; length(releve$Day)
\[1\] 61
&gt; ecarts &lt;- releve$Day\[2:61\]-releve$Day\[1:60\]

&gt; ecarts
\[1\] 50 57 55 13 30 42 67 24 17 33 60 31 13 8 7 14 32 14 29 16
\[21\] 11 15 37 21 26 28 35 28 28 21 14 14 15 16 18 14 17 12 16 11
\[41\] 17 11 16 15 12 13 16 19 14 13 17 13 21 19 9 14 22 19 24 11
&gt; range(ecarts)
\[1\] 7 67
&gt; mean(ecarts)
\[1\] 22.06667

Nous choisirions *a priori* **xmin = 1** (la première valeur), **deltat = 22** (l'écart moyen arroundi à l'entier le plus proche) et **n = 61** (le nombre de valeurs observées dans la série initiale), mais est-ce la meilleure combinaison pour obtenir un maximum de valeurs qui coïncident entre la série de départ mesurée et la série régulière finale calculée? Sachant que l’on est prêt à accepter une fenêtre de tolérance de 1 jour dans le temps de part et d’autre de la valeur régularisée (prendre un peu plus, car les fenêtres de tolérance n’incluent pas les bornes), il est possible de "screener" différentes combinaisons de **xmin** et de **deltat** autour de ces premières approximations à l’aide de la fonction ***regul.screen()*******:

&gt; regul.screen(releve$Day, xmin=0:11, deltat=16:27, tol=1.05)

$tol
d=16 d=17 d=18 d=19 d=20 d=21 d=22 d=23 d=24 d=25 d=26 d=27
1.07 1.06 1.06 1.05 1.05 1.05 1.05 1.04 1.04 1.04 1.04 1.04
$n
 d=16 d=17 d=18 d=19 d=20 d=21 d=22 d=23 d=24 d=25 d=26 d=27
x=0 83 78 74 70 67 64 61 58 56 54 51 50
x=1 83 78 74 70 67 64 61 58 56 53 51 50
x=2 83 78 74 70 67 64 61 58 56 53 51 50
x=3 83 78 74 70 67 63 61 58 56 53 51 49
x=4 83 78 74 70 67 63 61 58 56 53 51 49
x=5 83 78 74 70 67 63 61 58 56 53 51 49
x=6 83 78 74 70 66 63 60 58 55 53 51 49
x=7 83 78 74 70 66 63 60 58 55 53 51 49
x=8 83 78 74 70 66 63 60 58 55 53 51 49
x=9 83 78 74 70 66 63 60 58 55 53 51 49
x=10 83 78 74 70 66 63 60 58 55 53 51 49
x=11 83 78 74 70 66 63 60 58 55 53 51 49
$nbr.match
 d=16 d=17 d=18 d=19 d=20 d=21 d=22 d=23 d=24 d=25 d=26 d=27
x=0 9 12 13 6 8 5 5 5 10 12 10 12
x=1 10 15 12 6 8 9 5 9 11 9 7 12
x=2 13 9 11 10 11 11 8 11 14 9 8 13
x=3 13 10 10 7 9 12 10 11 13 6 8 7
x=4 14 9 8 10 7 9 11 7 11 5 7 9
x=5 10 11 6 6 7 6 8 6 8 10 5 5
x=6 12 13 4 7 7 1 7 8 5 7 6 5
x=7 9 11 5 8 11 4 9 6 6 9 7 2
x=8 10 11 9 11 11 14 12 6 4 4 7 5
x=9 13 11 9 13 12 19 9 6 4 6 4 3
x=10 14 10 13 13 13 17 7 10 4 5 5 7
x=11 14 9 14 12 8 9 3 9 5 5 7 6
$nbr.exact.match
 d=16 d=17 d=18 d=19 d=20 d=21 d=22 d=23 d=24 d=25 d=26 d=27
x=0 3 7 4 1 2 1 2 2 2 1 2 3
x=1 3 3 6 4 3 3 2 1 5 7 3 7
x=2 4 5 2 1 3 5 1 6 4 2 2 2
x=3 6 1 3 5 5 3 5 4 5 0 3 4
x=4 3 4 5 1 1 5 4 1 4 4 3 2
x=5 5 4 0 4 1 1 2 2 2 1 1 3
x=6 2 3 1 1 5 0 2 3 2 5 1 0
x=7 5 6 3 2 2 0 4 3 2 1 4 2
x=8 2 2 1 5 4 4 3 0 2 3 2 0
x=9 3 3 5 4 5 10 5 3 0 0 1 3
x=10 8 6 3 4 3 5 1 3 2 3 1 0
x=11 3 1 5 5 5 2 1 4 2 2 3 4

Le premier tableau ***$tol*** renvoie la tolérance pour chaque deltat *d*. Le second tableau indique pour chaque combinaison de deltat *d* et de xmin *x*, quelle est la valeur maximale acceptable pour *n* sans devoir faire d’extrapolation. Les deux tableaux suivants renvoient, pour les mêmes combinaisons *d* *versus* *x*, respectivement le nombre d’observations qui coïncident dans la fenêtre de tolérance, et le nombre d’observation qui coïncident exactement. Nous cherchons à optimiser ces deux critères. Clairement, la combinaison **xmin = 9**, **deltat = 21**, **n = 63**, avec **tol = 1.05** renvoie le meilleur résultat puisque 10 observations coïncident parfaitement et 19 sont incluses dans la fenêtre de tolérance. Cela signifie que nous n’aurons pas à interpoler 19/63, soit un peu plus de 30% des valeurs dans la série régulière. Nous pouvons ensuite visualiser l’effet d’une modification de la fenêtre de tolérance dans le nombre d’observations qu’il ne faudra pas interpoler à l’aide de ***regul.adj()*******:

&gt; regul.adj(releve$Day, xmin=9, deltat=21)
$params
xmin n deltat tol
 9 63 21 21
$match
\[1\] 59

$exact.match
\[1\] 10
$match.counts
0 1 2 3 4 5 6 7 8 9 10 12 13 14 15 16 19 20 Inf
10 19 21 23 24 29 32 41 46 48 49 51 53 55 56 57 58 59 63

Cette fonction, par défaut, retourne des résultats chiffrés et dessine un histogramme de la distribution des valeurs non interpolées en fonction d’une fenêtre de tolérance croissante. La première barre en bleu foncé représente le nombre de valeurs qui coïncident exactement, et la dernière barre rouge représente le nombre de valeurs qui doivent être interpolées quelle que soit la taille de la fenêtre de tolérance au maximum égale à **deltat** (c'est-à-dire, les trous dans la série) On voit que dans le cas présent, il n’est effectivement pas utile d’augmenter la fenêtre de tolérance d’un jour ou deux, car seul un petit nombre de valeurs seraient rajoutées. Il faudrait ouvrir cette fenêtre à 5-7 jours de part et d’autre de la date de mesure pour augmenter significativement le nombre de valeurs qui coïncident, mais nous pouvons considérer qu’une telle fenêtre est trop large par rapport à un pas de temps de 21 jours (cela dépend en fait de la nature des données traitées!).

> Il faut noter que comme les fonctions **regul.screen()** et **regul.adj()** ne manipulent que le vecteur temps, elles ne tiennent pas compte d’éventuelles valeurs manquantes dans la/les série(s) à régulariser. Si ces valeurs manquantes correspondent aux valeurs qui coïncident, on perd l’intérêt de ce "screening". Ces fonctions sont donc plutôt adaptées aux séries très irrégulières, mais avec peu de trous. De toute manière, elles sont totalement superflues pour des séries régulières à trous (le pas de temps est déjà fixé et connu!), et…on est bien mal embarqué si l’on possède des séries irrégulières à trous au départ!!!

Maintenant que l’on a déterminé les paramètres temporels optimaux, il ne nous reste plus qu’à effectuer la régularisation de ***releve*** (dans notre exemple, les 6 premières séries qui suivent les premières colonnes ***releve$Day*** et ***releve$Date***, donc, les colonnes 3 à 8). Pour cela, nous devons choisir une éventuelle transformation des données (***log()***, ou autre…), une méthode de régularisation (éventuellement différente pour chaque série), ainsi que les paramètres de régularisation (tel que la taille de la fenêtre pour la méthode des aires, ou la valeur de **f **pour la régularisation constante). Une fois notre choix effectué, nous lançons la régularisation par ***regul()******* avec tous les paramètres choisis:

&gt; rel.reg &lt;- regul(releve$Day, releve\[3:8\], xmin=9, n=63,
+ deltat=21, tol=1.05, methods=c("s","c","l","a","s","a"),
+ window=21)
&gt; rel.reg

Regulation of, by "method" :
 Astegla Chae Dity Gymn Melosul Navi
"spline" "constant" "linear" "area" "spline" "area"
Arguments for "methods" :
tol.type tol rule f periodic window split
"both" "1.05" "1" "0" "FALSE" "21" "100"
44 interpolated values on 63 ( 0 NAs padded at ends )
Time scale :
 start deltat frequency
9.00000000 21.00000000 0.04761905
Time units : days
call : regul(x = releve$Time, y = releve\[3:8\], xmin = 9, n = 63, deltat = 21, tol = 1.05, methods = c("s", "c", "l", "a", "s", "a"), window = 21)

Ainsi, 44 valeurs sont interpolées pour chaque série à l’aide de sa méthode respective, mais cela nous le savions déjà puisque les paramètres de temps pour les séries régulières ont été déterminés précédemment afin que 19 valeurs coïncident dans la fenêtre de tolérance choisie. De même, aucune valeur n’a été extrapolée (aucun **NA** n’a dû être ajouté si **rule = 1** comme ici) puisque **n** a été choisi judicieusement pour ne pas déborder de la série initiale. Il est possible d’avoir également la liste des points interpolés et leur valeur pour chaque série à l’aide de summary(rel.reg). La représentation graphique de n’importe laquelle de ces séries, superposée à la série de départ correspondante est simplement obtenue par ***plot()******* en précisant le numéro de la série à représenter:

&gt; plot(rel.reg, 5)

Les points interpolés sont représentés par une croix et les points qui coïncident à la série initiale (dans la fenêtre de tolérance) sont indiqués par une croix entourée d’un cercle. Les limites inférieures et supérieures des deux séries sont indiquées par des traits pointillés verticaux de couleur correspondante.

Il y a encore deux critères que nous n’avons pas considérés ici: (1) les autres séries (il faut qu’une base de temps commune soit apte à représenter correctement TOUTES les séries, pas seulement une seule), et (2) des contraintes sur le pas de temps d’un point de vue pratique (par exemple, on veut conserver un pas de temps bi-mensuel ou mensuel au lieu de 17 ou 21 jours parce qu’on cherche à mettre en évidence des fluctuations saisonnières et que l’on ne veut pas de décalages du pas de temps d’une année à l’autre).

En particulier, nous n’avons pas respecté la dernière contrainte. Avec une série pluriannuelle comme celle-ci, nous voudrions certainement étudier d’éventuelles variations saisonnières sur la série régularisée. Or, beaucoup de fonctions de R utilisables dans ce but nécessitent que l’unité de temps corresponde à la durée du cycle, c'est-à-dire, **"years"** ici. Bien entendu, il faut au moins 3 ou 4 mesures par unité de temps pour pouvoir étudier le cycle. R impose aussi que l’intervalle de temps soit un multiple exact de l’unité. Nous avons vu plus haut que, à plus ou moins un jour près, nous pouvons considérer qu’une année dure 365.25 jours. Toujours à un ou deux jours près dans la coupure à gauche et/ou à droite des mois, on peut considérer qu’un mois est équivalent à 1/12 d’une année, soit 365.25/12 = 30.44 jours. Si l’on veut s’en tenir à des valeurs bimensuelles ou "quinzaines", on doit donc considérer des intervalles **deltat** de 15.22 jours. L’utilisation de **tol** supérieur ou égale à 1 jour prend ici tout son sens! Ainsi, au lieu de 17 jours, nous serions contraints de prendre 15.22 jours comme intervalle pour pouvoir transposer le temps en **"years"**, avec une fréquence entière **frequency = 24** (24 "quinzaines" par an). Nous n’avons donc plus qu’un paramètre temporel libre pour ajuster au mieux la série régularisée afin d’interpoler le moins de points possibles. Les fonctions ***regul.screen()******* et ***regul.adj()******* nous renseignent toujours sur la valeur optimale de **xmin** dans ce cas précis (les résultats renvoyés ne sont pas imprimés):

&gt; regul.screen(releve$Day, weight, xmin=-1:14, deltat=365.25/24,
+ tol=2.2)
…
&gt; regul.adj(releve$Day, xmin=6, deltat=365.25/24)
…

Nous retiendrons ainsi: **xmin = 6**, **n = 87** et **tol  2.2**. Avec ces valeurs, 24 observations sur les 87 dans la série régulière ne devront pas être interpolées, soit environ 27%. Nous préférons préciser **frequency = 24** à **deltat** (qui serait une fraction puisqu’il vaut 1/24  0.0416667). La régularisation de la série ***Melosul***, avec une fréquence de 24 mesures par an exactement est donc obtenue par:

&gt; melo.regy &lt;- regul(releve$Day, releve$Melosul, xmin=6, n=87,
+ units="daystoyears", frequency=24, tol=2.2, methods="linear",
+ datemin="21/03/1989", dateformat="d/m/Y")

A 'tol' of 2.2 in 'days' is 0.00602327173169062 in 'years'
'tol' was adjusted to 0.00595238095238083

Le programme nous avertit que, puisque nous avons transformé l'échelle temporelle de **"days"** en **"years"** (argument **units="daystoyears"**), la valeur de **tol=2.2** qui était exprimée en jours devient 0.0602… lorsqu'elle est exprimée dans l'unité **"years"**. D'autre part, **tol** doit aussi être une fraction entière de 1/**frequency** (c'est-à-dire de **deltat**, voir ?regul). Or ce n'est pas le cas actuellement. La seconde ligne nous prévient donc que la valeur de **tol** a dû être ajustée à 0.00595 ans, soit 1/168 d'année, soit encore 1/168\*365.25 jours, c'est-à-dire 2.174 jours. Nous savions que **tol** allait être ajustée, puisque nous n’avons fourni qu’une valeur indicative. On précisera **datemin = "21/03/1989"** ou **datemin = releve$Date\[1\]** afin de caler l’axe temporel correctement. Nous obtenons le résultat suivant:

&gt; melo.regy
Regulation of, by "method" :
 Series
"linear"
Arguments for "methods" :
tol.type tol rule f periodic window split
"both" "0.00595” "1" "0" "FALSE" "15.3953488372093" "100"
63 interpolated values on 87 ( 0 NAs padded at ends )
Time scale :
 start deltat frequency
2.109000e+03 4.166667e-02 24
Time units : years
call : regul(x = releve$Time, y = releve$Melosul, xmin = 6, n = 87, units = "daystoyears", frequency = 24, datemin = releve$Date\[1\], tol = 2.2, methods = "linear")

&gt; plot(melo.regy, main="Regulation of Melosul")

On fait un tout petit peu moins bien qu’avec notre pas de temps de 17 jours dans la "capture" des pics et des creux, mais cette fois-ci, on obtient une échelle temporelle utilisable pour étudier les variations saisonnières. En fin de compte, la régularisation retenue sera un compromis entre les meilleures représentations pour les différentes séries et les contraintes imposées. Une fois que l’on est satisfait du résultat obtenu, il est extrêmement facile de convertir l’objet *‘regul’* issu de la régularisation en ‘time series’ régulière(s), c’est à dire, soit en objet *‘rts’* sous S+, soit en objet *‘ts’* sous R à l'aide de la fonction ***tseries()*******:

&gt; melo.ts &lt;- tseries(melo.regy)
&gt; is.tseries(melo.ts)
\[1\] TRUE

On peut aussi décider de n’extraire qu’une seule ou quelques séries régularisées contenues dans un objet *‘regul’* qui en contient plusieurs à l’aide de la méthode ***extract()*******:

&gt; melo.ts2 &lt;- extract(rel.reg, series="Melosul")

A partir d’ici, nous disposons de tous les outils de traitement de séries spatio-temporelles de R, ainsi que des routines spécialisées de PASTECS pour continuer l’analyse: analyse spectrale, lissage, filtrage, décomposition, tendance générale ou saisonnière, etc. (voir chapitres suivants).

> Les quatre fonctions suivantes (**regconst()**, **reglin()**, **regspline()** et **regarea()**) sont utilisées par **regul()**, mais sont aussi fournies séparément pour permettre à l’utilisateur avancé de réaliser ses propres routines de régularisation indépendamment de la fonction **regul()** et de l’objet ‘regul’ correspondant.

### Régularisation par valeur constante

Il s’agit de la méthode la plus simple de régularisation (mais aussi la moins puissante). Les valeurs interpolées *X*<sub>*j*</sub> aux temps *T*<sub>*j*</sub>* *sont calculées comme suit, si les valeurs mesurées qui encadrent *X*<sub>*j*</sub> dans la série irrégulière initiale sont *x*<sub>*i*-1</sub> et *x*<sub>*i*</sub> aux temps *t*<sub>*i*-1</sub> et *t*<sub>*i*</sub> (*i* est choisi tel que *t*<sub>*i*-1</sub> &lt; *T*<sub>*j*</sub>  *t*<sub>*i*</sub>).

avec *f* une constante comprise entre 0 et 1 et qui donne plus de poids à la valeur mesurée à gauche (*f* &lt; 0.5), ou à droite (*f* &gt; 0.5) de la valeur à interpoler. Avec *f* = 1 on a une fonction d’interpolation continue à gauche, avec *f* = 0 on a une fonction continue à droite et avec *f* = 0.5 on obtient la moyenne des deux valeurs encadrantes comme interpolation sur tout l’intervalle. Cette méthode est surtout utile avec *f* = 0, si la série de départ a été compressée avec un algorithme de type *RLE* ("Run-Length Encoding") qui reprend chaque valeur et l’intervalle sur lequel elle reste constante, au lieu de considérer un pas de temps constant.

<span id="anchor-5"></span>Exemple:

La série ***Melosul***** **du jeu de données ***releve*** est régularisée avec la méthode de valeur constante comme suit:

&gt; data(releve)
&gt; reg &lt;- regconst(releve$Day, releve$Melosul)

On peut tracer un graphique de la série de départ et de la série régularisée pour comparaison visuelle:

&gt; plot(releve$Day, releve$Melosul, type="l")
&gt; lines(reg$x, reg$y, col = 2)

La série de départ apparaît en noir, et la série régularisée en rouge. Lorsqu’une interpolation est nécessaire, la valeur immédiatement à gauche dans la série de départ est utilisée. A noter toutefois que cette méthode est incluse dans ***regul()*** qui offre plus de facilités pour le diagnostic et l’extraction des séries temporelles après régularisation. On peut visualiser les points de la série de départ superposés à la fonction continue calculée qui sert à la régularisation comme suit:

&gt; plot(releve$Day, releve$Melosul, type="p")
&gt; lines(regconst(releve$Day, releve$Melosul,
+ n=length(releve$Day)\*10), col=2, lty=2)

### Régularisation linéaire

Les points mesurés sont reliés par des segments de droite, et les valeurs interpolées sont obtenues à partir de la ligne brisée ainsi construite. L’équation qui donne les valeurs interpolées *X*<sub>*j*</sub> au temps *T*<sub>*j*</sub> à partir des valeurs encadrantes *x*<sub>*i*-1</sub>, *x*<sub>*i*</sub> aux temps respectifs *t*<sub>*i*-1</sub>, *t*<sub>*i*</sub> de la série irrégulière initiale est:

avec, pour tout *j* = 1…*p*, *i* est choisi tel que *t*<sub>*i*-1</sub> &lt; *T*<sub>*j*</sub>  *t*<sub>*i*</sub>.

<span id="anchor-6"></span>Exemple:

La série ***Melosul***** **du jeu de données** *****releve***** **est régularisée avec la méthode linéaire comme suit:

&gt; data(releve)\# Dans R uniquement
&gt; reg &lt;- reglin(releve$Day, releve$Melosul)

On peut tracer un graphique de la série de départ et de la série régularisée pour comparaison visuelle:

&gt; plot(releve$Day, releve$Melosul, type="l")
&gt; lines(reg$x, reg$y, col=4)

La série de départ apparaît en noir, et la série régularisée en bleu. L’interpolation est réalisée le long de la droite qui relie les deux valeurs encadrantes. Cette méthode est incluse dans ***regul()*** qui offre plus de facilités pour le diagnostic et l’extraction des séries temporelles après régularisation. On peut visualiser les points de la série de départ superposés à la fonction continue calculée qui sert à la régularisation comme suit:

&gt; plot(releve$Day, releve$Melosul, type="p")
&gt; lines(reglin(releve$Day, releve$Melosul,
+ n=length(releve$Day)\*10), col=4, lty=2)

### Régularisation par courbes splines

L’ajustement par des fonctions splines consiste à définir un polynôme dont les valeurs aux points d’observation coïncident avec les valeurs de l’échantillon. Le problème d’interpolation entre *p* points, donnés par une fonction *g*(*x*) dont les dérivées d’ordre 1 à *k* (avec 1  *k*) sont continues, peut être résolu en minimisant des fonctions du type:

avec: *a* &lt; *x*<sub>1</sub> &lt; … &lt; *x*<sub>*p*</sub> &lt; *b*

Plus on est exigeant sur l’ordre *k* des dérivées que l’on souhaite continues ainsi que sur le degré du polynôme *g*(*x*) choisi et plus l’expression de la fonction spline devient complexe. Nous considérerons des fonctions splines cubiques, c’est-à-dire constituées de polynômes du troisième degré dont les deux premières dérivées sont continues.

Soit *n* valeurs temporelles successives *t*<sub>1</sub>, *t*<sub>2</sub>, …, *t*<sub>*n*</sub> ainsi que les observations correspondantes *x*<sub>1</sub>, *x*<sub>2</sub>, …, *x*<sub>*n*</sub> de la série irrégulière initiale. L’algorithme de calcul est le suivant:

-   On calcule les écarts entre les abscisses temporelles: *H*<sub>*i*</sub> = *t*<sub>*i*</sub> – *t*<sub>*i*-1.</sub>
-   On construit un vecteur colonne des différences *V * tel que:

et une matrice carrée *M* telle que:

-   On calcule les dérivées secondes aux points expérimentaux. Pour cela il faut résoudre le système *M* . *DS* = *V* ou encore *DS* = *M*<sup>-1</sup> . *V*. On obtient ainsi dans le vecteur *DS* les dérivées secondes aux points d’origine. On attribue des dérivées secondes nulles aux points extrêmes de la courbe (argument **method = "fmm"** de ***regspline()***), ou on considère que la courbe forme une boucle (dernière valeur = première valeur, argument **method = "periodic"**).
-   On calcule la fonction spline pour chaque valeur de temps *T*<sub>*j*</sub> de la série régularisée selon un pas préalablement choisi. En appelant *X*<sub>*j*</sub>, les ordonnées calculées de la série régularisée correspondant à ces temps *T*<sub>*j*</sub>, on a:

avec *i* choisi tel que *t*<sub>*i-1*\\ </sub> &lt; *T*<sub>*j*</sub>* *  *t*<sub>*i*</sub> pour chaque valeur de *j *= 1…*p*.

> Etant donné que les courbes splines peuvent dans certains cas produire des valeurs plus grandes ou plus petites que les valeurs extrêmes de la série initiale, la méthode utilisée dans PASTECS recadre ces extrémas pour éviter qu’ils ne soient plus grands ou plus petits que les valeurs extrêmes de la série de départ. Cela évite, par exemple, d’obtenir des valeurs négatives pour des comptages dans certains cas.

##### Exemple {-}

La série ***Melosul***** **du jeu de données ***releve*** est régularisée avec des splines comme suit:

&gt; data(releve)\# Dans R uniquement
&gt; reg &lt;- regspline(releve$Day, releve$Melosul)

On peut tracer un graphique de la série de départ et de la série régularisée pour comparaison visuelle:

&gt; plot(releve$Day, releve$Melosul, type="l")
&gt; lines(reg$x, reg$y, col=3)

La série de départ apparaît en noir, et la série régularisée en vert. Les valeurs interpolées le sont le long de segments de courbes appelés splines. A noter toutefois que cette méthode est incluse dans ***regul()*** qui offre plus de facilités pour le diagnostic et l’extraction des séries temporelles après régularisation. On peut visualiser les points de la série de départ superposés à la fonction continue calculée qui sert à la régularisation comme suit:

&gt; plot(releve$Day, releve$Melosul, type="p")
&gt; lines(regspline(releve$Day, releve$Melosul,
+ n=length(releve$Day)\*10), col=3, lty=2)

### Régularisation par la méthode des aires

Une autre méthode d’interpolation linéaire correspond à l'interpolation par les aires décrites par Fox (1972). Cette méthode permet d'estimer les valeurs manquantes, mais plus généralement de substituer à une série irrégulière, des valeurs régulièrement espacées dans le temps en prenant en compte toutes les valeurs mesurées dans un intervalle constant à gauche et à droite de la valeur interpolée. Contrairement aux autres méthodes de régularisation, la courbe obtenue ne passe pas nécessairement par les points observés intiaux, c’est-à-dire qu’un lissage est effectué simultanément à l’interpolation. Ce lissage est d’autant plus important que la fenêtre d’interpolation est grande.

Soit une série d’observations *x*<sub>1</sub>, *x*<sub>2</sub>, ..., *x*<sub>*n*</sub> aux temps *t*<sub>1</sub>, *t*<sub>2</sub>,..., *t*<sub>*n*</sub>. On considère que chaque valeur *x*<sub>*i*</sub> représente une estimation des valeurs comprises dans l'intervalle de longueur , c’est-à-dire que l’on considère en fait des **aires** successives rectangulaires, plutôt que des observations ponctuelles discontinues dans le temps.

Un intervalle de temps *U* (une fenêtre), est choisi de préférence plus grand que la moyenne des durées minimums séparant les observations successives sur l'ensemble des séries initiales traitées. Considérons des fenêtres *U*<sub>*j*</sub> centrées autour des valeurs *T*<sub>*j*</sub> choisies pour la série régulière calculée. Ces fenêtres *U*<sub>*j*</sub> sont limitées par les dates *U*<sub>-*j*</sub> = *T*<sub>*j*</sub> – *U*/2 et *U*<sub>+*j*</sub> = *T*<sub>*j*</sub> + *U*/2. Le cas le plus simple correspond à la coïncidence entre le point au centre de la fenêtre et la date d'observation. Dans ce cas il n'est pas effectué de régularisation et la valeur observée reste inchangée (on peut aussi décider d’effectuer le calcul sur ce point à l’aide de l’option `interp = TRUE`, voir description de la fonction ***regarea()***). Dans les autres cas, l'interpolation prend en compte tous les intervalles *f*<sub>*i*</sub> inclus dans la fenêtre. Soit la fraction de *F*<sub>*i*</sub> appartenant à la fenêtre *U*<sub>*j*</sub>. Pour chaque observation, trois cas peuvent apparaître:

1.  les observations telles que (aire complètement inclue dans la fenêtre),
2.  les observations telles que (aire totalement en dehors de la fenêtre),
3.  les observations telles que (intervalle coupant un bord de la fenêtre).

On a: . La valeur interpolée est donnée par .

La figure illustre ce procédé. Il faut estimer la valeur *X*<sub>*j*</sub> au temps *T*<sub>*j*</sub>. Les points *x*<sub>*i*-2</sub>, *x*<sub>*i*-1</sub> et *x*<sub>*i*</sub> sont inclus dans la fenêtre. Les points *x*<sub>*i*-3</sub> et *x*<sub>*i*+1</sub> sont les proches à l'extérieur. Les valeurs réelles observées sont: *x*<sub>*i*-3</sub> = 10, *x*<sub>*i*-2</sub> = 5, *x*<sub>*i*-1</sub> = 9, *x*<sub>*i*</sub> = 3, *x*<sub>*i*+1</sub> = 6, et les correspondants: 0, 4, 6, 7, 1.

On a: .

D'où: *X*<sub>*j*</sub> = (10 . 0 + 5 . 4 + 9 . 6 + 3 . 7 + 6 . 1) / 18 = 5,6

Notons qu'une valeur nulle interpolée correspond soit:

-   au cas où toutes les observations *x*<sub>*i*</sub> satisfaisant sont égales à 0,
-   à la coïncidence de dates entre un 0 observé et le milieu de la fenêtre: *T*<sub>*j*</sub> = *t*<sub>*i*</sub> avec *x*<sub>*i*</sub> = 0 lorsque les données qui coïncident ne sont pas interpolées (argument `interp = FALSE` de ***regarea()***).

L’algorithme utilisé dans l'ancien PASSTEC 2000 correspond à peu de choses près à cette description. Le code se caractérise par des boucles (sur toutes les valeurs extrapolées) et de nombreux tests de conditions. L’algorithme a dû donc être totalement réécrit en calcul matriciel pour la librairie PASTECS. Quatre vecteurs sont créés: *t*<sub>-</sub>, *t*<sub>+</sub> et *U*<sub>-</sub>, *U*<sub>+</sub>. Les vecteurs *t*<sub>-</sub> et *t*<sub>+</sub> contiennent respectivement les bornes inférieures et supérieures de chaque aire correspondant aux points *x*<sub>*i*</sub> mesurés. Leur dimension est donc *n*, le nombre de points dans la série initiale. De même, les vecteurs *U*<sub>-</sub> et *U*<sub>+</sub> contiennent les bornes inférieures et supérieures des fenêtres entourant chaque point *X*<sub>*j*</sub> à extrapoler au temps *T*<sub>*j*</sub>. Leur dimension est *p*, le nombre de points à obtenir dans la série régularisée. Les vecteurs *t*<sub>-</sub> et *t*<sub>+</sub> sont recopiés *p* fois en colonnes, de manière à former deux matrices *n*  *p* (lignes  colonnes). De même, les vecteurs *U*<sub>-</sub> et *U*<sub>+</sub> sont recopiés *n* fois en lignes pour former également deux matrices de dimension *n * *p*. On peut montrer que les pour toutes les valeurs à extrapoler sont obtenus dans les colonnes respectives de *F* calculé comme suit:

Et ce, à condition que:

-   toutes les valeurs contenues dans les vecteurs *t*<sub>-</sub>, *t*<sub>+</sub>, *U*<sub>-</sub>, *U*<sub>+</sub> soient positives (il suffit de soustraire la plus petite valeur de temps rencontrée à chaque vecteur, si cette valeur est inférieures à 0, pour obtenir cette condition),
-   toutes les valeurs négatives obtenues pour les *F*<sub>*ij*</sub> soient remplacées par des zéros (elles correspondent à des plages totalement en dehors de la fenêtre *U*<sub>*j*</sub>).

Par conséquent, les valeurs interpolées sont obtenues dans le vecteur *X*<sub>*j*</sub> par:

où *i* est l’indice des lignes et *j* est l’indice des colonnes pour la matrice *F* (somme par colonne du produit *x*<sub>*i*</sub>.*F*<sub>*ij*</sub> divisé par la taille de fenêtre *U*).

L’inconvénient de cette méthode est de produire 4 matrices de très grandes tailles *n*  *p* pour des séries longues. De plus, comme la plupart des *F*<sub>*ij*</sub> sont nuls (en dehors de *U*) pour de très longues séries de taille nettement supérieure à la taille de la fenêtre *U*, cette méthode utilise une grande quantité de mémoire vive et de calcul dans pareil cas. La solution est de diviser la série en sous-unités plus petites et d’effectuer l’extrapolation sur chaque sous-unité indépendamment le long de la "diagonale" *j*  *i*. Nous avons déterminé empiriquement que, sous S+ comme sous R, la taille de sous-unité qui optimise la vitesse de calcul est d’environ 100 valeurs interpolées calculées à la fois. Cette taille de sous-unité est toutefois ajustable par l’argument **split** de la fonction ***regarea()***. Si le nombre de valeurs dans la série initiale est sensiblement égal au nombre de valeurs extrapolées, les matrices ont une taille d’environ 100  100, soit 10000 éléments. Notre machine de référence (Pentium II 500 Mhz avec 128 Mo de mémoire vive) est capable de régulariser la série ***Melosul*** de la matrice exemple ***releve*** (200 valeurs extrapolées, et fenêtre de 50 jours) en 0.175 sec avec `split = 100`, contre 2.68 sec avec `split = 1` (correspondant à peu près à la méthode de l'ancien PASSTEC 2000 de calcul valeur par valeur).

##### Exemple {-}

La série ***Melosul***** **du jeu de données ***releve*** est régularisée avec la méthode des aires, en utilisant une fenêtre de 25, puis de 50 jours) comme suit:

&gt; data(releve)
&gt; reg &lt;- regarea(releve$Day, releve$Melosul, window=25)
&gt; reg2 &lt;- regarea(releve$Day, releve$Melosul, window=50)

On peut tracer un graphique de la série de départ et des deux séries régularisées pour comparaison visuelle:

&gt; plot(releve$Day, releve$Melosul, type="l")
&gt; lines(reg$x, reg$y, col=2)
&gt; lines(reg2$x, reg2$y, col=4)

La série de départ apparaît en noir, la série régularisée avec une fenêtre de 25 jours est tracée en rouge, et celle régularisée avec une fenêtre de 50 jours, en bleu. Lorsqu’une interpolation est nécessaire, une moyenne des aires incluses dans la fenêtre est réalisée. Plus cette fenêtre est large, plus les valeurs interpolées seront "lissée" par rapport à la série de départ. A noter toutefois que cette méthode est incluse dans ***regul()*** qui offre plus de facilités pour le diagnostic et l’extraction des séries temporelles après régularisation. Les deux graphiques suivants montrent les fonctions continues calculées qui servent à la régularisation de l'exemple précédent pour respectivement un fenêtre de 25 jours (en rouge) et de 50 jours (en bleu):

&gt; plot(releve$Day, releve$Melosul, type="p")
&gt; lines(regarea(releve$Day, releve$Melosul, window=25,
+ n=length(releve$Day)\*10), col=2, lty=2)
&gt; plot(releve$Day, releve$Melosul, type="p")
&gt; lines(regarea(releve$Day, releve$Melosul, window=50,
+ n=length(releve$Day)\*10), col=4, lty=2)

### Conversion en séries régulières

Comme nous l'avons vu au paragraphe présentant les fonctions générales de régularisation, la fonction ***tseries()******* permet de convertir un objet *‘regul’* issu d’une régularisation (de même d’ailleurs qu’un objet *‘tsd’* issu d’une décomposition, voir plus loin) en objets *‘ts’* contenant **toutes** les séries. Pour extraire **une partie** des séries présentes dans l’objet *‘regul’*, utiliser plutôt la méthode ***extract()******* de l’objet. Enfin, la fonction ***is.tseries()******* est utile pour tester si un objet est une série régulière. Elle est équivalent à la fonction ***is.ts()******* proposées en standard dans R.

## Récapitulatif des exercices

Dans ce module 5, vous aviez à réaliser les exercices suivants\ :

`r show_ex_toc()`
