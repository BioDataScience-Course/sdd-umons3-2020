# Séries chronologiques II {#series2}

```{r setup, results='hide', warning=FALSE, include=FALSE}
SciViews::R
```

##### Objectifs {-}

- Être capable de décomposer une série régulière en ses différentes composantes, tendances, cycles et bruit blanc.

- Pouvoir régulariser une série irrégulière dans le but de l'analyser ensuite.

##### Prérequis {-}

Ce module nécessite que vous maîtrisiez parfaitement toutes les notions et techniques abordées dans le précédent module.

## Décomposition de séries temporelles

Toutes les techniques présentées dans cette partie ont pour but de décomposer des séries spatio-temporelles régulières (donc, à pas de temps constant et sans trous) en deux ou plusieurs composantes. On extrait soit une composante issue d’un filtrage ou d’un lissage, ou alors correspondant à un modèle (variation cyclique, tendance linéaire, polynomiale, exponentielle, etc, …). Quel que soit le nombre de composantes obtenues, la dernière est toujours le résidu (**residuals**), c’est-à-dire, la fraction de la série initiale (**series**) qui ne se retrouve dans aucune des autres composantes (**components**).

Pour un modèle **additif**, on aura\ :

**residuals** = **series** – **components**

Un tel modèle est également appelé **linéaire** lorsque chaque composante est une combinaison linéaire de la ou des variables dépendantes, en l’occurrence pour les séries spatio-temporelles, le temps ou l’espace, ou les deux ensemble.

Un modèle **multiplicatif** se définira selon un schéma équivalent, mais dans ce cas, c’est le produit des composantes qui donnera la série. Une des composantes (la série filtrée, la série désaisonnalisée ou la tendance générale, selon le contexte), est alors exprimée dans les mêmes unités que la série de départ, alors que les autres sont des facteurs multiplicatifs adimensionnels. Il est possible de transformer un modèle multiplicatif en un modèle additif en passant aux logarithmes, puisque le log d’un produit est égal à la somme des log des facteurs. Si l’opération donne une combinaison linéaire des log des facteurs, on dit que l’on a **linéarisé** le modèle par la transformation logarithmique.

On pourrait encore définir des modèles **mixtes** où certaines composantes sont multiplicatives alors que d’autres sont additives, mais il n’y a alors plus moyen de les linéariser. Pour cette raison, de tels modèles sont très peu répandus. La nature de la ou des composantes dépend du traitement effectué. Par exemple pour un **filtrage**, il s’agit de la série filtrée; pour une **stationnarisation**, il s’agit de la tendance générale extraite, pour une **désaisonnalisation**, il s’agit du cycle saisonnier, etc.

### Fonction générale de décomposition

Dans {PASTECS}pastecs}, toutes les techniques de décomposition de série fonctionnent selon le même canevas et renvoient toutes un objet **tsd** (*time series decomposition*). Cet objet contient à la fois les composantes de la ou des séries qui ont été décomposées, des informations sur la méthode utilisée, ainsi que des données supplémentaires qui servent au diagnostic du traitement réalisé.

La fonction centrale qui effectue le traitement sur une ou plusieurs séries et renvoie un objet **tsd** est `tsd()`. Son argument `method=` permet de sélectionner une méthode de traitement à appliquer à toutes les séries à décomposer. Son argument `type=` indique si le modèle est additif ou multiplicatif. Cette fonction appelle d’autres fonctions plus simples `decxxx()` où `xxx` est le nom de la méthode (ex\ : `decmedian()` pour la méthode des médianes mobiles, par exemple). Les fonctions `decxxx()` ne peuvent traiter qu’une série à la fois, et ne sont normalement pas appelées directement par l'utilisateur. Toutefois, étant donné qu’elles renvoient aussi des objets **tsd**, l’utilisateur avancé peut les préférer pour des questions de performance ou de simplicité dans ses propres scripts.

Une fois qu’une décomposition est réalisée, il est possible d’extraire les différentes composantes (grâce à `tseries()`) ou une partie d’entre elles (en utilisant la méthode `extract()`) et de les retransformer en objet séries régulières (objet **ts** ou **mts**). On peut ainsi appliquer de nouvelles méthodes de décomposition sur un ou plusieurs composants et approfondir l’analyse des séries par des traitements de décomposition en cascade jusqu’à ce qu’on ait extrait et analysé toute l’information pertinente contenue dans ces séries.

##### Exemple {-}

On peut régulariser les séries 3 à 8 du jeu de données `releve` à l’aide de la méthode linéaire comme suit\ :

&gt; data(releve)
&gt; rel.regy &lt;- regul(releve$Day, releve\[3:8\], xmin=6, n=87,
+ units="daystoyears", frequency=24, tol=2.2, methods="linear",
+ datemin="21/03/1989", dateformat="d/m/Y")

Ensuite, on va transformer les séries régularisées en objets ‘time series’ utilisables par `tsd()`\ :

&gt; rel.ts &lt;- tseries(rel.regy)

Nous pouvons à présent effectuer simultanément une décomposition de toutes les séries contenues dans ***rel.ts***. Par exemple, pour extraire un cycle saisonnier par la méthode LOESS, nous entrons:

&gt; rel.dec &lt;- tsd(rel.ts, method="loess", s.window=13, trend=FALSE)
&gt; rel.dec
Call:
tsd(x = rel.ts, method = "loess", s.window = 13, trend = FALSE)
Series:
\[1\] "Astegla" "Chae" "Dity" "Gymn" "Melosul" "Navi"
Components for Astegla
\[1\] "seasonal" "residuals"
Components for Chae
\[1\] "seasonal" "residuals"
Components for Dity
\[1\] "seasonal" "residuals"
Components for Gymn
\[1\] "seasonal" "residuals"
Components for Melosul
\[1\] "seasonal" "residuals"
Components for Navi
\[1\] "seasonal" "residuals"

On peut maintenant afficher le graphe correspondant à la décomposition de l’une des séries, par exemple ***Melosul***:

&gt; plot(rel.dec, series=5, col=1:3)

Enfin, l’extraction de composantes de certaines séries et leur transformation en nouveaux objets *‘ts*’ se fait comme suit (par exemple, on extrait uniquement la composante ***deseasoned*** des séries 3 à 6, et puis on affiche les 10 premiers éléments contenus dans les ‘time series’ extraites):

&gt; rel.des &lt;- extract(rel.dec, series=3:6, components="deseasoned")
&gt; rel.des\[1:10,\]
 Dity.deseasoned Gymn.deseasoned Melosul.deseasoned Navi.deseasoned
\[1,\] -1856.450218 728.08507 11232.346 1263.204
\[2,\] -326.203029 925.87962 4857.551 2193.775
\[3,\] -5.397257 958.98367 12218.422 2141.072
\[4,\] 305.799403 725.75255 19485.965 2042.397
\[5,\] 510.738352 389.66511 19241.367 2524.486
\[6,\] 673.469410 46.52693 16485.644 2800.179
\[7,\] 666.811431 -205.96157 17601.801 2505.898
\[8,\] 668.430789 928.90368 16997.344 2273.421
\[9,\] 694.149206 -1949.28870 17367.641 1813.558
\[10,\] 696.564421 -1890.15039 16520.938 1603.852

Ces objets peuvent ensuite être analysés avec les outils destinés aux séries régulières (autocorrélation, analyse spectrale, etc.) ou être décomposés à leur tour par un nouvel appel à ***tsd()*******. Par exemple, nous pouvons éliminer la tendance générale présente dans les résidus (**stationnarisation**) par la méthode des moyennes mobiles, en leur appliquant une nouvelle fois ***tsd()***, avec l’argument **method = "average"** (notez que la série ***Melosul*** est en position 3 maintenant):

&gt; rel.des.dec &lt;- tsd(rel.des, method="average", order=2, times=10)
&gt; plot(rel.des.dec, series=3, col=c(2,4,6))

Dans ce dernier graphe, ***series*** correspond donc à la composante ***deseasoned ***issue du premier traitement, ***filtered*** correspond à la tendance générale présente dans cette série désaisonnalisée et ***residuals*** correspond à la partie (pseudo-)stationnaire qu’il reste de la série après élimination d’un cycle saisonnier (premier traitement) et d’une tendance générale (second traitement) selon un modèle additif (la somme de toutes les composantes redonne la série de départ). Pour le graphe, on aurait donc pu préciser **labels = c("deseasoned", "trend", "residuals")** pour être plus clair. Notons au passage qu’il est possible de faire d’autres types de graphiques à l’aide de la méthode ***plot()*******. Par exemple, pour représenter de manière superposée la série désaisonnalisée et la tendance qui en est extraite (ce qui est peut-être plus parlant dans ce cas), nous pourrions entrer:

&gt; plot(rel.des.dec, series=3, col=c(2,4), stack=FALSE,
+ resid=FALSE, labels=c("without season cycle", "trend"),
+ lpos=c(0,55000))

On peut continuer l’analyse en approfondissant de plus en plus les décompositions et les analyses des composantes jusqu’à ce que l’on considère avoir extrait toute l’information contenue dans la ou les séries initiales. Par exemple, on pourrait effectuer une analyse spectrale des résidus après élimination du cycle saisonnier et de la tendance générale:

&gt; rel.res2 &lt;- extract(rel.des.dec, components="residuals")
&gt; spectrum(rel.res2\[,3\], spans=c(5,7))

A l’évidence, il y a encore un signal cyclique de fréquence 2,5-3 dans les résidus qui n’a pas encore été extrait.

> Les sept fonctions suivantes (**decdiff()**, **decaverage()**, **decmedian()**, **decevf()**, **decreg()**, **decensus() **et** decloess()**) sont utilisées par **tsd()**, mais sont aussi fournies séparément pour permettre à l’utilisateur avancé de réaliser ses propres routines de décomposition indépendamment de la fonction **tsd()**. Nous en verrons quelques unes par la suite dans ce cours.

### Filtrage d’une série spatio-temporelle

Le filtrage d’une série ou chronique consiste à remplacer chaque valeur de cette série par une combinaison de ses diverses valeurs. Un filtrage est dit** linéaire** lorsqu’il utilise une combinaison linéaire des valeurs de cette série:

où *X*<sub>*t*</sub> est la série de départ, *Y*<sub>*t*</sub> est la série filtrée. Les méthodes des différences ou la méthode des moyennes mobiles sont des filtres linéaires, alors que la médiane mobile, par exemple, est un filtre non linéaire. Lorsque le filtre conduit à réduire la variance en éliminant notamment les très hautes fréquences, on parle aussi de **lissage** du signal (smoothing en anglais).

### Filtrage linéaire par la méthode des différences

La méthode des différences a pour but d’éliminer la tendance. Ce n’est valable que si la série a une tendance monotone et non en "dents de scie". Autrement dit, cela suppose que la série est autocorrélée positivement. Pour décrire la méthode, définissons d’abord la notion **d’opérateur de retard**:

Si *k* = 0, la série est inchangée. L’intérêt de ce formalisme est que *L* peut être traité comme une variable. Ainsi, on aura l’équivalence:

En effet, appelons *Y*<sub>*t*</sub> = *L*<sup>*s\\ *</sup>*X*<sub>*t*</sub> = *X*<sub>*t-s*</sub>. Comme *L*<sup>*k*</sup>* Y*<sub>*t*</sub> = *Y*<sub>*t-k*</sub>, on a bien *L*<sup>*k*</sup>*L*<sup>*s*</sup>*X*<sub>*t*</sub> = *X*<sub>*t-k-s*</sub>, qui correspond à l’expression ci-dessus.

Considérons l’opérateur polynomial . Cet opérateur est la différence première:

Les différences d’ordre *r* (successives) sont définies par:

où désigne les combinaisons simples de *i* termes pris *r* à *r*. Par exemple pour les différences secondes:

La transformation de *X*<sub>*t*</sub> enélimine totalement ou en partie la tendance. On sait en effet qu’un polynôme de degré *p* présente la propriété que sa *p*<sup>ème</sup> différence finie est une constante et que les différences d’ordre *p* + 1, *p* + 2, … sont nulles.

La méthode des différences ne permet donc pas d’évaluer la tendance mais de l’éliminer; elle est extrêmement courante lorsqu’on désire se rapprocher de façon rapide et simple de la stationnarité, en répétant le procédé si nécessaire. Elle est ainsi employée très souvent préalablement à l’analyse spectrale.

La méthode des différences, dans une version légèrement différente, est également efficace pour éliminer une tendance sinusoïdale. Si on dispose par exemple de séries pluriannuelles avec un pas d’observation mensuel, dans la mesure où on considère que la variabilité saisonnière peut être modélisée par une sinusoïde (ce qui n’est pas le cas en général, car souvent les cycles annuels biologiques sont "télescopés"\ : la période de reproduction printanière est souvent décalée d’une année sur l’autre…), alors on remplace les données *X*<sub>*t*</sub> par les écarts aux moyennes des mois respectifs. Si la série était une sinusoïde, un tel filtrage aurait pour effet de la transformer en une droite. Supposons que l’on ait *n* années, la valeur désaisonnalisée *Y*<sub>*t*</sub> au mois *i* s’écrira:

 où *X*<sub>*i*</sub> sont les valeurs respectives rencontrées au mois *i* pendant les *n* années considérées. Cette méthode n’est pas implémentée dans ***decdiff()***, mais elle est disponible dans ***decloess()******* avec les arguments **order = "periodic"** et **trend = FALSE**.

##### Exemple {-}

On peut calculer la différence du second ordre (**lag = 1**) pour la série ***ClausocalanusB*** en log du jeu de données ***marbio***, puis tracer le graphe de la filtration par:

&gt; data(marbio)
&gt; ClausoB.ts &lt;- ts(log(marbio$ClausocalanusB + 1))
&gt; ClausoB.dec &lt;- decdiff(ClausoB.ts, lag=1, order=2, ends="fill")
&gt; plot(ClausoB.dec, col=c(1,4,2), xlab="stations")

### Filtrage linéaire par les moyennes mobiles

La moyenne mobile (MB) remplace chaque observation par une moyenne pondérée sur les termes situés de part et d’autre de cette observation. Par exemple, si la MB est calculée sur trois termes, on aura:

*M*<sub>*t*</sub> = *b*<sub>-1</sub>.*X*<sub>*t-*1</sub> + *b*<sub>0</sub>.*X*<sub>*t*</sub> + *b*<sub>+1</sub>.*X*<sub>*t+*1</sub>
*M*<sub>*t*+1</sub> = *b*<sub>-1</sub>.*X*<sub>*t*</sub> + *b*<sub>0</sub>.*X*<sub>*t*+1</sub> + *b*<sub>+1</sub>.*X*<sub>*t*+2
</sub>etc.

La somme de ces poids égale l’unité. On a donc les relations:

Lorsque tous les coefficients de pondération sont égaux, la MB est dite **simple**. La constante *b*<sub>**</sub> ne dépend plus du rang **, et on a:

La valeur 2*k* + 1 est appelée **bande de lissage** de la MB **d’ordre** *k*.

Appliquer successivement un filtrage par les MB revient à appliquer un filtrage par une MB unique. On démontre que si, après avoir effectué une MB sur 2*k* + 1 termes, on applique une seconde MB sur 2*l* + 1 termes, la série finale peut être obtenue directement à partir de la série initiale par une MB à 2(*k* + *l*) + 1 termes:

On peut définir les poids *d*<sub>**</sub> de cette MB unique à partir des poids *b*<sub>**</sub> et *c*<sub>**</sub> des deux MB successives:

1.  Poids de MB1:
2.  Poids de MB2:
3.  Poids de MB12:

Le produit matriciel *BC* est une matrice qui contient autant de diagonales qu’il y a de coefficients à calculer, soit 2(*k* + *l*) + 1.

La **variance** de la série filtrée vaut:

Ainsi, une MB simple de 13 termes, appliquée à une série aléatoire donne une série de même moyenne, mais dont la variance est réduite de 1/13. Si une MB est soumise à la restriction que la somme de ses poids vaut 1, la somme des carrés des poids sera minimale lorsque ceux-ci seront égaux à 1. C’est donc une MB simple qui réduit le plus efficacement la variabilité d’une série.

Lorsque l’on effectue une MB, on se trouve confronté à un problème d’estimation des termes extrêmes de la série lissée. L’estimation peut se faire en effectuant une MB progressivement moins étendue. Les deux valeurs les plus imprécises, le premier et le dernier point faisant l’objet d’une moyenne à seulement *k* +1 termes au lieu de 2*k* + 1.

Une technique plus précise consiste à rajouter des valeurs fictives avant de passer au lissage. Supposons que l’on désire effectuer une MB d’ordre 2 sur une série à *n* termes. Deux valeurs au début, et deux valeurs à la fin ne peuvent être estimées. On considère alors la moyenne des deux premiers termes de la série que l’on va placer deux fois au début de la série. De même, on calcule la moyenne des deux derniers termes, et on rajoute cette valeur au bout de la série. La série avant le filtrage MB est alors à *n* + 4 termes. Elle va donc permettre une estimation, imparfaite, mais suffisante en pratique.

Une MB calculée sur 2*k* + 1 termes a la propriété **d’éliminer le cycle de même période**. La **fréquence de coupure du spectre** (voir chapitre analyse des séries spatio-temporelles) est en effet égal à:

Ainsi, si on dispose d’une série pluriannuelle avec des observations mensuelles, une MB simple centrée de 12 mois élimine la variation saisonnière. Cependant, on ne peut pas centrer les observations dans une fenêtre contenant un nombre pair de termes. C’est pourquoi on va considérer un lissage avec une fenêtre à 13 termes. Le filtre de **désaisonnalisation** s’écrira:

Comme il faut considérer le poids de 12 valeurs et non de 13, les observations extrêmes dans la fenêtre comptent seulement pour moitié. Naturellement, 6 valeurs au début et 6 valeurs à la fin de la série ne pourront être estimées par cet algorithme. La méthode d’addition des valeurs au début et à la fin de la série ne s’applique pas ici si on a à estimer 6 mois successifs au début et à la fin d’une série pluriannuelle. Si la série comporte un grand nombre d’années, on pourra préalablement rajouter les valeurs des 6 derniers mois de la première année au début et les valeurs des 6 premiers mois de la dernière année à la fin.

Quand on effectue une MB d’ordre 6 pour éliminer un cycle annuel, la variance de la série lissée est pondérée d’un facteur égal à:

Cela signifie que la variance est réduite de 28% (). Cette MB a la propriété d’éliminer les fonctions périodiques de période égale à 12, 6, 4, 3, 12/5 et 2. Elle épargne les fonctions linéaires.

L’application du filtre des MB induit un mouvement périodique appelé **effet Slutsky-Yule**. La série engendrée par une MB sur un processus purement aléatoire ne l’est plus. La période approximative *T* de ce mouvement est déterminé par la corrélation entre les termes successifs et peut se calculer par la relation cos(*T*) = *r*(1). Si par exemple le coefficient d’autocorrélation d’ordre 1, *r*(1), d’une MB à 13 termes est 12/13, ce filtre engendre des oscillations parasites qui obéissent à cos(*T*) = 12/13, soit *T*  22.6° ou en mois, *T*  360/22.6  16 mois. L’application répétée infinie de MB conduit à une sinusoïde pure.

A noter que sous S+/R, on dispose d'autres fonctions très puissantes de lissage, notamment le ***kernel smoothing*** et le ***supersmoother*** de Friedmann. Toutes les techniques de décompositions non prévues dans ***tsd()*** peuvent toutefois être utilisées à l'aide de la méthode **"reg"**, puisque cette dernière demande la série initiale et la série traitée (sans se préoccuper du traitement effectué par ailleurs) pour créer l'objet *'tsd'*.

##### Exemple {-}

On peut filtrer 10 fois de suite la série ***ClausocalanusB*** (en log) du jeu de données ***marbio*** par une moyenne mobile simple et centrée d’ordre 2, puis tracer le graphe de la filtration par:

&gt; data(marbio)
&gt; ClausoB.ts &lt;- ts(log(marbio$ClausocalanusB + 1))
&gt; ClausoB.dec &lt;- decaverage(ClausoB.ts, order=2, times=10,
+ sides=2, ends="fill")
&gt; plot(ClausoB.dec, col=c(1,3,2), xlab="stations")

Ici, on voit mieux le résultat sur un graphique superposé sur lequel les résidus ne sont pas représentés:

&gt; plot(ClausoB.dec, col=c(1,3), xlab="stations", stack=FALSE,
+ resid=FALSE, lpos=c(53,4.3))

On peut aussi désaisonnaliser la série ***Melosul*** du jeu de données ***releve*** (après régularisation) par une moyenne mobile, bien que la méthode LOESS soit plus efficace à ce niveau:

&gt; data(releve)
&gt; melo.regy &lt;- regul(releve$Day, releve$Melosul, xmin=9, n=87,
+ units="daystoyears", frequency=24, tol=2.2, methods="linear",
+ datemin="21/03/1989", dateformat="d/m/Y")
&gt; melo.ts &lt;- tseries(melo.regy)
&gt; melo.dec &lt;- decaverage(melo.ts, order="periodic", times=1,
+ sides=2, ends="periodic")
&gt; plot(melo.dec, col=c(1,6,2))

Comme on peut le constater, le "cycle saisonnier" est de forme quelconque dans ***residuals***. Il s’agit en fait de toutes les oscillations de fréquences de 12 mois, ainsi que les plus petites harmoniques qui ont été éliminées, pour ne laisser que les oscillations plus longues dans ***filtered***. Attention toutefois à l’effet Slutsky-Yule si le filtre est appliqué plusieurs fois.

### Filtrage non linéaire par les médianes mobiles

Au lieu d'utiliser des moyennes mobiles, on peut appliquer des médianes mobiles. Dans ce cas, on considérera également une fenêtre impaire à *k* termes et on calculera sa médiane:

*Z*<sub>*t*</sub> = médiane(*X*<sub>*t-k*</sub>, …, *X*<sub>*t*</sub>, …, *X*<sub>*t+k*</sub>)

On démontre qu’une expression comme est minimisée quand ** est égal à la médiane de *Z*<sub>*t*</sub>.

Le lissage (ou filtrage) des médianes mobiles peut être itérativement répété. Ce procédé dit des "running medians" (R) converge rapidement vers une courbe invariante. La notation conventionnelle est la suivante:

1.  3RSR: lissage à 3 termes jusqu’à obtention de la convergence. Une médiane 3RSR correspond aux valeurs qui minimisent l’expression

1.  53RSR: lissage double, soit un de 3 termes, et ensuite un de 5 termes jusqu’à convergence. Néanmoins, la convergence n’est pas absolument garantie si la série est longue.
2.  4(3RSR)2H: ce lissage est préconisé par Tukey (1977). Il est généralement appliqué deux fois: on calcule les résidus d’un premier lissage, on effectue un second lissage du même type sur ces résidus, et on additionne les deux séries lissées ensemble.

Ce filtre peut être appliqué soit pour lisser, soit surtout pour segmenter une série. La courbe obtenue est voisine de celle de départ (différent d’avec les moyennes mobiles), sauf en ce qui concerne les grands pics et les grands creux de la série. Ceux-ci sont ignorés et on constate un lissage en forme de plateaux successifs. En effet, si on prend trois points successifs, on obtiendra fatalement un plateau en conservant la valeur centrale. Si on répète ce procédé, on tend à créer des plateaux de plus de deux valeurs. Ce lissage peut aussi servir à détecter un petit nombre de classes et à reconnaître des discontinuités ponctuelles.

##### Exemple: {-}

On peut filtrer 10 fois de suite (bien plus que ce qu’il ne faut pour atteindre la stabilisation) la série ***ClausocalanusB*** en log du jeu de données ***marbio***, par une médiane mobile d’ordre 2, puis tracer le graphe de la filtration par:

&gt; data(marbio)
&gt; ClausoB.ts &lt;- ts(log(marbio$ClausocalanusB + 1))
&gt; ClausoB.dec &lt;- decmedian(ClausoB.ts, order=2, times=10,
+ ends="fill")
&gt; plot(ClausoB.dec, col=c(1,4,2), xlab="stations")

Les différents "paliers" sont ici mis en évidence. Un graphique superposé (mais sur lequel on ne trace ni les résidus **resid = FALSE**, ni la courbe d’origine **col = 0**) et sur lequel on représente aussi les séparations des masses d’eau, identifiées de manière indépendante, fait apparaître une certaine relation:

&gt; plot(ClausoB.dec, col=c(0,2), xlab="stations",
+ stack=FALSE, resid=FALSE)
&gt; lines(c(17,17), c(0,10), col=4, lty=2)
&gt; lines(c(25,25), c(0,10), col=4, lty=2)
&gt; lines(c(30,30), c(0,10), col=4, lty=2)
&gt; lines(c(41,41), c(0,10), col=4, lty=2)
&gt; lines(c(46,46), c(0,10), col=4, lty=2)
&gt; text(c(8.5,21,27.5,35,43.5,57), 8.7, labels=c("Peripheral Zone",
+ "D1", "C", "Front", "D2", "Central Zone"))

### Estimation de la tendance par régression

Les méthodes de filtrage présentées ci-dessus permettent d’extraire ou d’éliminer une tendance de forme quelconque (on n’a pas d’idée *a priori* de sa forme; on ne cherche pas à la modéliser). Les méthodes d’estimation par régression, au contraire, font appel à des hypothèses très fortes quant à sa nature: on cherche à ajuster une ou plusieurs équations fonctionnelles à la tendance de la série.

L’idée simple pour estimer une tendance générale est de vérifier son ajustement par une droite, une parabole, un polynôme d’ordre plus élevé simulant un mouvement pseudo-cyclique de forte période. Ces techniques reposent sur l’algorithme des moindres carrés: on minimise les carrés d’écarts entre les données observées et un polynôme de degré fixé à l’avance. L’estimation des paramètres se fait en considérant un système d’équations partielles.

Pour un processus *X*<sub>*t*</sub>, la régression simple en fonction du temps est de forme: *X*<sub>*t*</sub> = *a*. *t* + *b, *pour un polynôme d’ordre 2: *X*<sub>*t*</sub> = *a*<sub>1</sub>.*t*<sup>2</sup> + *a*<sub>2</sub>.*t* + *b*, et ainsi de suite. Une ambiguïté vient de ce que, même si un ajustement par un polynôme d’ordre 5 par exemple, semble très bien décrire visuellement la tendance générale, l’ajustement simple peut être lui aussi déjà hautement significatif. Pour savoir si un degré supérieur est nécessaire, il faut tester si le coefficient de régression partielle attaché à ce degré est significativement différent de 0. Ajoutons qu’un ajustement n’est valable théoriquement que si les erreurs sont indépendantes entre elles et possèdent une distribution normale, cas malheureusement peu fréquent avec des séries spatio-temporelles!

Quoi qu’il en soit, ce ne sont pas les résultats statistiques qui serviront de base à la définition de la tendance générale: l’écologiste, suivant le problème à résoudre, le fait de disposer d’un modèle de variation d’une forme donnée au départ, en fonction de l’échelle de l’étude ou selon sa propre expérience, reste le seul maître de la décision.

On pourrait imaginer bien d’autres formes de tendance générale (exponentielle, logistique, etc.). S+ et R offrent énormément de possibilités pour la définition et l’ajustement de modèles linéaires et non-linéaires. Certains de ces modèles prennent en compte une autocorrélation entre les observations, aspect fondamental dans le cadre de l’analyse de séries spatio-temporelles. Nous renvoyons le lecteur intéressé à la documentation en ligne de ces logiciels respectifs pour plus d’informations à ce sujet.

La tendance générale est parfois liée à l’évidence à un cycle (annuel, lunaire, etc.). La régression sinusoïdale permet de tester statistiquement, et de définir un modèle de variation rigoureusement périodique. Naturellement, ce modèle doit être utilisé avec discernement. Ainsi, même si la variabilité saisonnière existe pour des abondances d’espèces, elle ne s’ajuste pas forcément à une sinusoïde si les amplitudes maximums sont décalées d’une année à l’autre, d’un mois ou plus. Et si alors on étudie les écarts entre les valeurs observés et le modèle, ceux-ci ne correspondent plus à une variabilité indépendante de la variation saisonnière. Ils ne représentent que l’inadéquation du modèle choisi, qui lui, est strictement périodique.

L’équation fonctionnelle sera donnée par:

En effectuant le changement de variable suivant:

on se ramène au système linéaire:

identique au cas de la multirégression (système d’équations normales à résoudre).

On doit aussi estimer le déphasage, c’est-à-dire trouver la valeur de **, soit par:

On en déduit:

Avec le calcul des résidus, on peut tester l’ajustement par analyse de la variance. En appelant variation la somme des carrés des écarts, le modèle se décompose en variation sinusoïdale + variation résiduelle. Sous réserve de normalité, on pourra tester l’ajustement par un test *F* de Fisher (tableau de l’ANOVA; rapport de la variance expliquée par la régression sur la variance résiduelle). Sinon, des tests non paramétriques faciliteront l’interprétation (analyse de variance non paramétrique, corrélation de rangs).

<span id="anchor-26"></span>Exemples:

Modèle linéaire de la tendance

On peut effectuer une régression linéaire sur la série ***Density*** du jeu de données ***marphy*** afin de l'utiliser comme estimation de la tendance générale dans une décomposition de cette série spatio-temporelle. La variable indépendante (x) est le temps que l'on reconstitue à partir de la série spatio-temporelle à l'aide de la fonction ***time()***. Ensuite, on effectue la régression linéaire à l'aide de la fonction ***lm()******* qui demande de spécifier le modèle sous forme **Y ~ X**, avec **Y** étant la variable dépendante et **X** étant la variable indépendante. La méthode ***summary()******* retourne un rapport détaillé sur cette régression:

&gt; data(marphy)
&gt; density &lt;- ts(marphy\[, "Density"\])
&gt; Time &lt;- time(density)
&gt; density.lin &lt;- lm(density ~ Time)
&gt; summary(density.lin)

Call:
lm(formula = density ~ Time)

Residuals:
 Min 1Q Median 3Q Max
-0.052226 -0.018014 0.001945 0.017672 0.058895

Coefficients:
 Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) 2.884e+01 6.585e-03 4379.27 &lt;2e-16 \*\*\*
Time 3.605e-03 1.659e-04 21.73 &lt;2e-16 \*\*\*
---
Signif. codes: 0 \`\*\*\*' 0.001 \`\*\*' 0.01 \`\*' 0.05 \`.' 0.1 \` ' 1

Residual standard error: 0.02685 on 66 degrees of freedom
Multiple R-Squared: 0.8774, Adjusted R-squared: 0.8755
F-statistic: 472.3 on 1 and 66 DF, p-value: 0

 Notons que le tableau de l'ANOVA indique qu'aussi bien la pente **Time** que l'ordonnée à l'origine **(Intercept)** sont très hautement significatifs. Mais rappelons que nous violons ici une hypothèse de base de la régression linéaire puisque les termes d'erreurs présentent très probablement une certaine autocorrélation et ne sont donc pas totalement indépendants entre eux! On peut cependant considérer les résultats de l'ANOVA comme indicatifs, tout en gardant à l'esprit qu'ils sont biaisés. Les valeurs de densité correspondant à ce modèle sont récupérées à l'aide de la méthode ***predict()*******, et nous pouvons maintenant superposer le modèle au graphe de la série originale.

&gt; xreg &lt;- predict(density.lin)
&gt; plot(density)
&gt; lines(xreg, col=3)
&gt; title("Linear model for trend in 'density'")

La décomposition de la série ***density*** est réalisée grâce à ***decreg()******* comme suit. Nous obtenons un objet *'tsd'* qui nous est maintenant familier, ainsi que le type de représentation graphique à l'aide de la méthode ***plot() ***qui lui est associé:

&gt; density.dec &lt;- decreg(density, xreg)
&gt; plot(density.dec, col=c(1, 3, 2), xlab="stations")

Modèle polynomial de la tendance

Nous pouvons bien entendu nous demander si une droite est le meilleur modèle qui soit pour représenter la tendance générale dans cette série. Ainsi, par exemple, on aurait pu également choisir un polynôme d'ordre 2 pour représenter cette tendance, ce qui donne:

&gt; density.poly &lt;- lm(density ~ Time + I(Time^2))
&gt; summary(density.poly)

Call:
lm(formula = density ~ Time + I(Time^2))

Residuals:
 Min 1Q Median 3Q Max
-0.067079 -0.010665 0.001498 0.014777 0.045341

Coefficients:
 Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) 2.880e+01 8.374e-03 3439.404 &lt; 2e-16 \*\*\*
Time 6.593e-03 5.600e-04 11.773 &lt; 2e-16 \*\*\*
I(Time^2) -4.330e-05 7.866e-06 -5.505 6.73e-07 \*\*\*
---
Signif. codes: 0 \`\*\*\*' 0.001 \`\*\*' 0.01 \`\*' 0.05 \`.' 0.1 \` ' 1

Residual standard error: 0.02234 on 65 degrees of freedom
Multiple R-Squared: 0.9164, Adjusted R-squared: 0.9138
F-statistic: 356.2 on 2 and 65 DF, p-value: 0

Une régression polynomiale s'obtient en effectuant une régression linéaire sur des termes successifs x, x<sup>2</sup>, x<sup>3</sup>, x<sup>4</sup>,… Pour un polynôme d'ordre 2, on s'arrête au second terme. La formule à utiliser est donc du type: **Y ~ X + I(X^2)** (nous sommes obligés d'utiliser la fonction ***I()******* pour "protéger" notre terme **X^2**, étant donné qu'il ne s'agit plus simplement d'une variable, mais d'une expression (nous renvoyons le lecteur intéressé par cela à l'aide en ligne de la fonction ***lm()***). Avec toutes les précautions que nous avons déjà évoquées pour l'interprétation du tableau de l'ANOVA, nous observons que ce terme supplémentaire **I(Time^2)** est lui aussi très fortement significatif. Un polynôme d'ordre 2 semble donc parfaitement indiqué pour représenter la tendance générale dans cette série. Voyons ce que cela donne graphiquement:

&gt; xreg2 &lt;- predict(density.poly)
&gt; plot(density)
&gt; lines(xreg2, col=3)
&gt; title("Order 2 polynome for trend in 'density'")

C'est effectivement beaucoup mieux que le modèle linéaire. La décomposition de la série à l'aide de ***decreg()******* se fait comme précédemment:

&gt; density.dec2 &lt;- decreg(density, xreg2)
&gt; plot(density.dec2, col=c(1, 3, 2), xlab="stations")

Modèle non linéaire de la tendance

Nous pourrions bien entendu tester également des polynômes d'ordre 3, 4,…, mais nous nous arrêterons-là dans notre exemple pour tester une autres possibilité: l'utilisation d'un modèle non linéaire pour représenter la tendance générale dans cette série, par exemple, une courbe logistique:

&gt; density.logis &lt;- nls(density ~ SSlogis(Time, Asym, xmid, scal))
&gt; summary(density.logis)

Formula: density ~ SSlogis(Time, Asym, xmid, scal)

Parameters:
 Estimate Std. Error t value Pr(&gt;|t|)
Asym 29.13767 0.03855 755.908 &lt; 2e-16 \*\*\*
xmid -210.29714 41.78382 -5.033 4.07e-06 \*\*\*
scal 47.28910 10.33741 4.575 2.20e-05 \*\*\*
---
Signif. codes: 0 \`\*\*\*' 0.001 \`\*\*' 0.01 \`\*' 0.05 \`.' 0.1 \` ' 1

Residual standard error: 0.023 on 65 degrees of freedom

Correlation of Parameter Estimates:
 Asym xmid
xmid -0.9722
scal 0.9811 -0.999

Nous utilisons maintenant la fonction ***nls()******* en lieu et place de la fonction ***lm()***. La définition du modèle est quelque peu différente: **Y ~ model(X, params,…)**, avec **model()**, une fonction non linéaire. Lorsque la fonction est capable de calculer elle-même ses paramètres de départ, ces derniers ne doivent pas êtres fournis et l'utilisation de la fonction ***nls()*** est alors presque aussi simple que celle de ***lm()***, mais il n'en va pas toujours ainsi. Voyez l'aide en ligne de ***nls()*** pour plus de renseignements sur l'utilisation de la régression non linéaire dans S+ ou R. Nous observons que, ici encore tous les paramètres semblent très hautement significatifs dans le modèle. Nous ne commenterons pas plus loin les résultats renvoyés par la méthode *** summary()*******. L'extraction des valeurs prédites par le modèle, le graphe superposé de la fonction de départ et la décomposition de la série s'obtiennent comme avec un modèle linéaire:

&gt; xregl &lt;- predict(density.logis)
&gt; plot(density)
&gt; lines(xregl, col=3)
&gt; title("Logistic model for trend in 'density'")

&gt; density.decl &lt;- decreg(density, xregl)
&gt; plot(density.decl, col=c(1, 3, 2), xlab="stations")

Modèle sinusoidal de tendance cyclique

Pour terminer, nous allons illustrer l'utilisation d'un modèle sinusoidal pour représenter une composante saisonnière dans une série. Pour ce faire, nous utiliserons une série artificielle périodique:

&gt; tser &lt;- ts(sin((1:100)/12\*pi)+rnorm(100, sd=0.3), start=c(1998,
+ 4), frequency=24)

Comme nous l'avons vu dans l'introduction théorique, un changement de variable nous permet de linéariser un modèle sinusoïdal, de sorte que nous pouvons utiliser la fonction ***lm()***, plus simple d'utilisation que la fonction ***nls()***:

&gt; Time &lt;- time(tser)
&gt; tser.sin &lt;- lm(tser ~ I(cos(2\*pi\*Time)) + I(sin(2\*pi\*Time)))
&gt; summary(tser.sin)

Call:
lm(formula = tser ~ I(cos(2 \* pi \* Time)) + I(sin(2 \* pi \* Time)))

Residuals:
 Min 1Q Median 3Q Max
-0.74682 -0.19634 0.03170 0.19409 0.64224

Coefficients:
 Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) -0.02944 0.02875 -1.024 0.308
I(cos(2 \* pi \* Time)) -0.51424 0.04111 -12.508 &lt;2e-16 \*\*\*
I(sin(2 \* pi \* Time)) 0.91312 0.04019 22.720 &lt;2e-16 \*\*\*
---
Signif. codes: 0 \`\*\*\*' 0.001 \`\*\*' 0.01 \`\*' 0.05 \`.' 0.1 \` ' 1

Residual standard error: 0.2871 on 97 degrees of freedom
Multiple R-Squared: 0.8719, Adjusted R-squared: 0.8692
F-statistic: 330.1 on 2 and 97 DF, p-value: 0

Conformément à notre construction de la série aléatoire, les paramètres **cos(…)** et **sin(…)** sont tous deux très significatifs, mais pas l'ordonnée à l'origine **(Intercept)**. Le tracé des graphes et la décomposition de la série à l'aide de ce modèle se font de manière similaire aux exemples précédents:

&gt; tser.reg &lt;- predict(tser.sin)
&gt; tser.dec &lt;- decreg(tser, tser.reg)
&gt; plot(tser.dec, col=c(1, 4), xlab="stations", stack=FALSE,
+ resid=FALSE, lpos=c(0, 4))

&gt; plot(tser.dec, col=c(1, 4, 2), xlab="stations")

### Décomposition par LOESS

LOESS est une méthode de régression polynomiale locale. Pour chaque valeur *X*<sub>*t*</sub> d’une série *X*, on va considérer les *k* voisins à gauche et à droite, éventuellement pondérés (par exemple par la distance les séparant de *X*<sub>*t*</sub>). On effectue une régression par les moindres carrés d’un polynôme d’ordre *p* (habituellement *p* vaut 1 ou 2), et on récupère la valeur prédite au temps *t* (voir Cleveland *et al*, 1992). Si l’ordre *p* du polynôme vaut 0, on se ramène à une moyenne mobile.

Avec un choix judicieux de *k*, et éventuellement en réitérant le processus, on va pouvoir extraire la tendance générale. Comme avec la méthode des moyennes mobiles, on va aussi pouvoir désaisonnaliser une série en prenant une fenêtre égale à un an, donc, en considérant 12 termes pour des données mensuelles (13 en fait pour une fenêtre centrée sur les observations de la série). Une combinaison de ces deux traitements va permettre de décomposer une série en tendance, cycle saisonnier et résidus selon un modèle additif. On peut aussi n’extraire que le cycle saisonnier et les résidus (surtout utile lorsque la tendance générale *est* périodique sur un an).

Comme les résidus ont rarement une distribution normale et ne sont souvent pas indépendants entre eux dans les séries, les hypothèses de base indispensables pour la régression à l’aide de la méthode des moindres carrés sont violées la plupart du temps. Ainsi, pour obtenir une estimation plus exacte des composantes, il est possible d’utiliser une méthode de régression plus robuste (*i.e.*, moins sensible à la violation de ces hypothèses de base). On peut par exemple décider de pondérer les valeurs de manière inversement proportionnelle à leur influence respective sur la régression. Ainsi, une valeur qui, à elle seule, aurait tendance à tirer la courbe vers elle (valeur isolée, très éloignée du nuage formé par toutes les autres) recevra une pondération très faible afin de minimiser son effet.

##### Exemple {-}

L’utilisation de ***decloess()******* pour extraire un cycle saisonnier à partir de la méthode des différences a déjà été présentée dans l’exemple de ***tsd()***. Nous le reprenons ici, et nous explorerons ensuite d’autres valeurs pour les paramètres, afin de voir leurs effets sur la décomposition:

&gt; data(releve)
&gt; melo.regy &lt;- regul(releve$Day, releve$Melosul, xmin=9, n=87,
+ units="daystoyears", frequency=24, tol=2.2, methods="linear",
+ datemin="21/03/1989", dateformat="d/m/Y")
&gt; melo.ts &lt;- tseries(melo.regy)
&gt; melo.dec &lt;- decloess(melo.ts, s.window="periodic")
&gt; plot(melo.dec, col=1:3)

Avec l’option **s.window = "periodic"**, la forme de la composante cyclique saisonnière est quelconque, mais est reproduite à l’identique pour toutes les années (puisque la même moyenne mensuelle est utilisée pour chaque mois de chaque année). Avec l’option **s.window = 13** et **s.degree = 0**, on obtient pratiquement le même résultat. Par contre, avec **s.degree = 1**, on va permettre une variation de la composante cyclique saisonnière d’une année sur l’autre:

&gt; melo.dec2 &lt;- decloess(melo.ts, s.window=13, s.degree=1)
&gt; plot(melo.dec2, col=1:3)

Une étude de la cohérence (voir analyse des séries spatio-temporelle, paragraphe analyse spectrale croisée) entre les deux composantes ***deseasoned*** et ***seasonal*** sous R donne:

&gt; melo.tsd &lt;- tseries(melo.dec2)
&gt; melo.spc &lt;- spectrum(melo.tsd, spans=c(3,3), plot=FALSE)
&gt; plot(melo.spc, plot.type="c")

On peut voir que la cohérence est très faible entre les deux séries, c’est le principe même de la décomposition saisonnière. Enfin, sous R uniquement, on peut également extraire la tendance (selon un modèle additif), simultanément à un cycle saisonnier qui se répète d’année en année (**s.window = "periodic"**) ou qui évolue au cours du temps (**s.window  frequency**), en précisant **trend = TRUE**:

&gt; melo.dec3 &lt;- decloess(melo.ts, s.window="periodic", trend=TRUE)
&gt; plot(melo.dec3, col=c(1,4,3,2))

## Régularisation 

Les méthodes présentées dans cette partie permettent de transformer une série spatio-temporelle échantillonnée avec un pas de temps irrégulier ou présentant des trous, en une série régulière. L’idéal pour une série temporelle est d’être échantillonnée avec un **pas de temps** régulier. On parle alors d’une **série** régulière. Malheureusement, en biostatistique, on a rarement affaire à des séries parfaitement régulières, soit parce qu’il y a des observations manquantes, soit parce qu’on ne contrôle pas l’intervalle de temps. Dans ce cas, la régularisation est une opération préliminaire indispensable à beaucoup d’analyses de séries spatio-temporelles, que ce soit avec les outils PASTECS ou avec d'autres méthodes. Les objets **ts** doivent être *d’office* des séries spatio-temporelles régulières dans R. Cette étape est donc requise pour adapter des **data frames** avant de pouvoir les transformer en series temporelles\ ! Plusieurs méthodes de régularisation sont disponibles dans {pastecs}\ : régularisation par valeur constante (`regconst()`), linéaire (`reglin()`), par courbes splines (`regspline()`) et par la méthode des aires (`regarea()`). Afin de simplifier le travail de régularisation, et de faciliter ensuite la création des series temporelles, toutes ces fonctions ont été regroupées sous une fonction commune `regul()`.

```{block2, type = 'warning'}
Toutes ces méthodes sont à utiliser **avec précaution**. Il est très facile d’obtenir n’importe quoi si l’on choisit des mauvais paramètres\ ! Il n’y a pas de règle absolue dans leurs choix, étant donné qu’ils sont liés à la nature-même des données à régulariser. Par exemple, il faut être extrêmement attentif à ne pas régulariser à tort des données physico-chimiques (par exemple à l’aide de la méthode des aires avec une très grande fenêtre).
```

Quelle que soit la méthode utilisée, on doit veiller à obtenir un pas de temps de la série régulière aussi proche que possible du pas de temps (moyen) de la série échantillonnée. De même, on doit aussi essayer d’obtenir autant de valeurs non interpolées que possible dans la série régulière finale. Quant à l’extrapolation (obtenue à l’aide de l’argument `rule = 2`), elle est à proscrire. La fonction `regul()` admet beaucoup de paramètres. L’utilisateur est invité à essayer différents réglages sur ses propres données, et à visualiser le résultat à l’aide de `plot.regul()` et `hist.regul()` pour se familiariser avec ses possibilités.

Dans le cas où l’on voudrait condenser des données aléatoires (par exemple, transformer un échantillonnage mensuel en valeurs saisonnières ou des données quotidiennes en série par semaine ou par mois), il vaut mieux condenser les données en calculant des moyennes ou des médianes successives. On peut aussi régulariser la série avec le pas temps initial si ce dernier est un multiple du pas de temps visé, et utiliser ensuite la fonction `aggregate()` de R, que nous avons déjà utilisée.

### Choix du pas de temps

Dans le cas de séries initiales très irrégulières, les fonctions `regul.screen()` et `regul.adj()` permettent de déterminer quels sont les meilleurs paramètres temporels pour la série régularisée afin qu’un maximum d’observations de la série initiale coïncident avec les valeurs de temps échantillonnées pour la série régularisée. Une fenêtre de tolérance pour déterminer si les valeurs coïncident est utilisée. Celle-ci est calculée en interne à l’aide de la fonction `match.tol()` qui ne sera donc en principe pas utilisée directement. Un exemple d'utilisation de ces fonctions est donné dans le paragraphe suivant.

### Fonction de régularisation

Dans {pastecs}, la régularisation d’une ou plusieurs séries se fait grâce à la fonction `regul()` qui renvoie un objet **regul** (en fait, une liste contenant un **data frame** qui représente la ou les séries régularisées, accompagnées d’un certains nombres d’attributs supplémentaires qui permettent le diagnostic du traitement, ainsi que l’extraction aisée des objets time series **ts** à partir de cette matrice à l’aide de `extract()` ou `tseries()`. La méthode `summary()` résume le résultat de la régularisation. La méthode `specs()` récupère les spécifications d’un objet **regul** pour éventuellement les appliquer à une nouvelle régularisation. La méthode `plot()` permet de réaliser facilement le graphe superposé de la série initiale et de la série finale à des fins de comparaison. La méthode `lines()` permet de superposer à un graphe déjà tracé une autre série régularisée (par exemple pour comparer deux méthodes de régularisation entre elles). La méthode `identify()` permet de sélectionner des points sensibles lors de la régularisation directement sur le graphe. La méthode `hist()` permet de représenter les points qui correspondent entre la série originale et la série régularisée, en fonction de la tolérance choisie dans le temps pour déclarer que les données sont équivalentes. Pour l’utilisateur avancé, les fonctions de régularisations respectives `regconst()`, `reglin()`, `regspline()` et `regarea()` sont aussi disponibles mais ne prennent pas en charge tout le traitement annexe de diagnostic et d’extraction de séries chronologiques. Par contre, ces dernières fonctions peuvent s’avérer plus pratiques dans des scripts et programmes personnels.

Dans la plupart des logiciels, le temps est exprimé par rapport à une date de départ arbitraire (par exemple, le 1^er^ janvier 1970 par défaut dans R). Les dates antérieures à cette valeur de départ sont exprimées par des valeurs négatives. Les dates sont manipulées en interne comme des nombres décimaux classiques. La partie entière de la date représente le jour, la partie décimale représente l’heure. Par exemple, le nombre 1.5 représente le 2 janvier 1970, à 12:00:00 (le "second jour et demi" par rapport à la date de départ qui vaut 0) dans R, si l'origine temporelle n'a pas été redéfinie. Il est possible de connaître l'origine temporelle utilisée sur votre système grâce à la function `getOption()`, et la modifier en appelant la même fonction\ :

```{r}
options("chron.origin") # Regarde sa valeur
options(chron.origin = c(month = 1, day = 1, year = 1990))
options("chron.origin")
options(chron.origin = NULL) # Revient vers la valeur pas défaut
options("chron.origin")
```

Lorsque `chron.origin = NULL`, cela signifie que c'est la valeur par défaut du 01/01/1970 dans R qui est prise en compte.

```{block2, type = 'warning'}
Si vous définissez des dates, et puis que vous modifiez `chron.origin` entre deux calculs, les mêmes variables risquent de se référer à des dates différentes avant et après le changement! Pour éviter cela, et aussi pour prendre en compte les fuseaux horaires et les horaires été/hiver, R utilise un format plus complexe de dates appelé **POSIXt**. Nous invitons le lecteur intéressé à consulter l'aide en ligne sous R à la rubrique `?DateTimeClasses`.
```

Les séries temporelles dans R admettent toute autre représentation arbitraire du temps en mode décimal. On peut par exemple décider de décompter les secondes écoulées à partir d’un temps de référence à l’aide de la partie entière du nombre (dixièmes, centièmes, etc. de secondes pour la partie décimale). Cela pourra être renseigné par l’argument `units=` de `regul()` dans lequel on entrera `"sec"` au lieu de `"days"` par défaut. L'utilisateur peut donc se référer à n'importe quelle échelle de temps, avec une unité au choix qu'il peut préciser dans l'argument `units=`. Cependant, il est conseillé de s'en tenir aux unités reconnues par les fonctions de la libraire PASTECS, parce que cela permettra notamment un formattage idéal des unités dans les sorties graphiques à l'aide de la fonction `GetUnitText()` (utilisée en interne, et donc, normalement pas appelée directement par l'utilisateur). Ces unités reconnues sont\ : `"years"`, `"days"`, `"weeks"`, `"hours"`, `"min"` et `"sec"`.

Une échelle particulièrement utile pour manipuler des séries pluriannuelles est **"years"**. Dans ce mode, la partie entière représente l’année, et la partie décimale représente le mois, le jour, l’heure, etc… Ainsi par exemple, 1998.167 représente le 1<sup>er</sup> mars 1998 à 00:00:00 heures, soit 1998 + 2/12 (0/12 pour début janvier, 1/12 pour début février, 2/12 pour début mars, …, 11/12 pour début décembre). En général, on travaille sur des données mensuelles dans ce mode, et on arrondit au mois près. Il s’agit en réalité d’une approximation puisque tous les mois n’ont pas le même nombre de jours (ni toutes les années), mais c’est une représentation pratique qui permet de se débarrasser des tracas liés aux nombres de jours par mois, aux années bissextiles, etc… Dans la fonction *** regul()***** **il est possible de préciser **units = "daystoyears"** qui convertit automatiquement des dates classiques où l’unité est le jour dans le mode d’unité **"years"**.

> A noter que, d'une manière générale, l'analyse de cycles bien précis tels que l'effet saisonnier ou les cycles circadiens nécessitent d'exprimer le temps dans une unité qui correspond exactement au cycle étudié. Ainsi, les fonctions de décomposition des séries en composantes saisonnières (**decloess()** par exemple, voir chapitre décomposition des séries spatio-temporelles) imposent d'exprimer le temps en unité **"years"**. De même, il est fortement conseillé d'adopté l'unité **"days"** pour l'étude de cycles circadiens.

La fonction ***regul()*** offre encore une autre particularité. Elle permet de ne pas interpoler des données qui seraient présentes dans la série de départ, ***avec une fenêtre de tolérance******** réglable***. Ainsi par exemple, si une mesure est présente au jour 123, mais aucune au jour 124 dans la série initiale, et qu’il faut calculer une valeur dans la série régularisée au jour 124 (pas d’une semaine ou plus), on peut décider que la valeur échantillonnée au jour 123 est une très bonne estimation de la valeur au jour 124 et ne pas effectuer d’interpolation. La fenêtre dans laquelle il faut aller voir si une valeur mesurée existe est réglable par le paramètre **tol** de ***regul()***. De plus, on peut décider d’aller voir de chaque côté, uniquement à gauche, ou seulement à droite (voir paramètre **tol.type**). Ainsi, on peut accroître sensiblement le nombre de valeurs non interpolées dans la série régularisée, en considérant une certaine "souplesse" dans les dates entre les séries échantillonnées et régularisées. Si cela n'est pas simple, un exemple concret va clarifier les choses…

##### Exemple {-}

Chargeons le jeu de données ***releve*** à partir de la librairie PASTECS. Etant donné que cette série est très irrégulière comme on peut le voir ci-dessous, il est très difficile de décider quel est le meilleur pas de temps (**deltat**), le meilleur temps initial (**xmin**), ainsi que le nombre de mesures optimal (**n**) pour interpoler le moins de valeurs possibles:

&gt; data(releve)\# Dans R uniquement
&gt; releve$Day
\[1\] 1 51 108 163 176 206 248 315 339 356 389 449
\[13\] 480 493 501 508 522 554 568 597 613 624 639 676
\[25\] 697 723 751 786 814 842 863 877 891 906 922 940
\[37\] 954 971 983 999 1010 1027 1038 1054 1069 1081 1094 1110
\[49\] 1129 1143 1156 1173 1186 1207 1226 1235 1249 1271 1290 1314
\[61\] 1325
&gt; length(releve$Day)
\[1\] 61
&gt; ecarts &lt;- releve$Day\[2:61\]-releve$Day\[1:60\]

&gt; ecarts
\[1\] 50 57 55 13 30 42 67 24 17 33 60 31 13 8 7 14 32 14 29 16
\[21\] 11 15 37 21 26 28 35 28 28 21 14 14 15 16 18 14 17 12 16 11
\[41\] 17 11 16 15 12 13 16 19 14 13 17 13 21 19 9 14 22 19 24 11
&gt; range(ecarts)
\[1\] 7 67
&gt; mean(ecarts)
\[1\] 22.06667

Nous choisirions *a priori* **xmin = 1** (la première valeur), **deltat = 22** (l'écart moyen arroundi à l'entier le plus proche) et **n = 61** (le nombre de valeurs observées dans la série initiale), mais est-ce la meilleure combinaison pour obtenir un maximum de valeurs qui coïncident entre la série de départ mesurée et la série régulière finale calculée? Sachant que l’on est prêt à accepter une fenêtre de tolérance de 1 jour dans le temps de part et d’autre de la valeur régularisée (prendre un peu plus, car les fenêtres de tolérance n’incluent pas les bornes), il est possible de "screener" différentes combinaisons de **xmin** et de **deltat** autour de ces premières approximations à l’aide de la fonction ***regul.screen()*******:

&gt; regul.screen(releve$Day, xmin=0:11, deltat=16:27, tol=1.05)

$tol
d=16 d=17 d=18 d=19 d=20 d=21 d=22 d=23 d=24 d=25 d=26 d=27
1.07 1.06 1.06 1.05 1.05 1.05 1.05 1.04 1.04 1.04 1.04 1.04
$n
 d=16 d=17 d=18 d=19 d=20 d=21 d=22 d=23 d=24 d=25 d=26 d=27
x=0 83 78 74 70 67 64 61 58 56 54 51 50
x=1 83 78 74 70 67 64 61 58 56 53 51 50
x=2 83 78 74 70 67 64 61 58 56 53 51 50
x=3 83 78 74 70 67 63 61 58 56 53 51 49
x=4 83 78 74 70 67 63 61 58 56 53 51 49
x=5 83 78 74 70 67 63 61 58 56 53 51 49
x=6 83 78 74 70 66 63 60 58 55 53 51 49
x=7 83 78 74 70 66 63 60 58 55 53 51 49
x=8 83 78 74 70 66 63 60 58 55 53 51 49
x=9 83 78 74 70 66 63 60 58 55 53 51 49
x=10 83 78 74 70 66 63 60 58 55 53 51 49
x=11 83 78 74 70 66 63 60 58 55 53 51 49
$nbr.match
 d=16 d=17 d=18 d=19 d=20 d=21 d=22 d=23 d=24 d=25 d=26 d=27
x=0 9 12 13 6 8 5 5 5 10 12 10 12
x=1 10 15 12 6 8 9 5 9 11 9 7 12
x=2 13 9 11 10 11 11 8 11 14 9 8 13
x=3 13 10 10 7 9 12 10 11 13 6 8 7
x=4 14 9 8 10 7 9 11 7 11 5 7 9
x=5 10 11 6 6 7 6 8 6 8 10 5 5
x=6 12 13 4 7 7 1 7 8 5 7 6 5
x=7 9 11 5 8 11 4 9 6 6 9 7 2
x=8 10 11 9 11 11 14 12 6 4 4 7 5
x=9 13 11 9 13 12 19 9 6 4 6 4 3
x=10 14 10 13 13 13 17 7 10 4 5 5 7
x=11 14 9 14 12 8 9 3 9 5 5 7 6
$nbr.exact.match
 d=16 d=17 d=18 d=19 d=20 d=21 d=22 d=23 d=24 d=25 d=26 d=27
x=0 3 7 4 1 2 1 2 2 2 1 2 3
x=1 3 3 6 4 3 3 2 1 5 7 3 7
x=2 4 5 2 1 3 5 1 6 4 2 2 2
x=3 6 1 3 5 5 3 5 4 5 0 3 4
x=4 3 4 5 1 1 5 4 1 4 4 3 2
x=5 5 4 0 4 1 1 2 2 2 1 1 3
x=6 2 3 1 1 5 0 2 3 2 5 1 0
x=7 5 6 3 2 2 0 4 3 2 1 4 2
x=8 2 2 1 5 4 4 3 0 2 3 2 0
x=9 3 3 5 4 5 10 5 3 0 0 1 3
x=10 8 6 3 4 3 5 1 3 2 3 1 0
x=11 3 1 5 5 5 2 1 4 2 2 3 4

Le premier tableau ***$tol*** renvoie la tolérance pour chaque deltat *d*. Le second tableau indique pour chaque combinaison de deltat *d* et de xmin *x*, quelle est la valeur maximale acceptable pour *n* sans devoir faire d’extrapolation. Les deux tableaux suivants renvoient, pour les mêmes combinaisons *d* *versus* *x*, respectivement le nombre d’observations qui coïncident dans la fenêtre de tolérance, et le nombre d’observation qui coïncident exactement. Nous cherchons à optimiser ces deux critères. Clairement, la combinaison **xmin = 9**, **deltat = 21**, **n = 63**, avec **tol = 1.05** renvoie le meilleur résultat puisque 10 observations coïncident parfaitement et 19 sont incluses dans la fenêtre de tolérance. Cela signifie que nous n’aurons pas à interpoler 19/63, soit un peu plus de 30% des valeurs dans la série régulière. Nous pouvons ensuite visualiser l’effet d’une modification de la fenêtre de tolérance dans le nombre d’observations qu’il ne faudra pas interpoler à l’aide de ***regul.adj()*******:

&gt; regul.adj(releve$Day, xmin=9, deltat=21)
$params
xmin n deltat tol
 9 63 21 21
$match
\[1\] 59

$exact.match
\[1\] 10
$match.counts
0 1 2 3 4 5 6 7 8 9 10 12 13 14 15 16 19 20 Inf
10 19 21 23 24 29 32 41 46 48 49 51 53 55 56 57 58 59 63

Cette fonction, par défaut, retourne des résultats chiffrés et dessine un histogramme de la distribution des valeurs non interpolées en fonction d’une fenêtre de tolérance croissante. La première barre en bleu foncé représente le nombre de valeurs qui coïncident exactement, et la dernière barre rouge représente le nombre de valeurs qui doivent être interpolées quelle que soit la taille de la fenêtre de tolérance au maximum égale à **deltat** (c'est-à-dire, les trous dans la série) On voit que dans le cas présent, il n’est effectivement pas utile d’augmenter la fenêtre de tolérance d’un jour ou deux, car seul un petit nombre de valeurs seraient rajoutées. Il faudrait ouvrir cette fenêtre à 5-7 jours de part et d’autre de la date de mesure pour augmenter significativement le nombre de valeurs qui coïncident, mais nous pouvons considérer qu’une telle fenêtre est trop large par rapport à un pas de temps de 21 jours (cela dépend en fait de la nature des données traitées!).

> Il faut noter que comme les fonctions **regul.screen()** et **regul.adj()** ne manipulent que le vecteur temps, elles ne tiennent pas compte d’éventuelles valeurs manquantes dans la/les série(s) à régulariser. Si ces valeurs manquantes correspondent aux valeurs qui coïncident, on perd l’intérêt de ce "screening". Ces fonctions sont donc plutôt adaptées aux séries très irrégulières, mais avec peu de trous. De toute manière, elles sont totalement superflues pour des séries régulières à trous (le pas de temps est déjà fixé et connu!), et…on est bien mal embarqué si l’on possède des séries irrégulières à trous au départ!!!

Maintenant que l’on a déterminé les paramètres temporels optimaux, il ne nous reste plus qu’à effectuer la régularisation de ***releve*** (dans notre exemple, les 6 premières séries qui suivent les premières colonnes ***releve$Day*** et ***releve$Date***, donc, les colonnes 3 à 8). Pour cela, nous devons choisir une éventuelle transformation des données (***log()***, ou autre…), une méthode de régularisation (éventuellement différente pour chaque série), ainsi que les paramètres de régularisation (tel que la taille de la fenêtre pour la méthode des aires, ou la valeur de **f **pour la régularisation constante). Une fois notre choix effectué, nous lançons la régularisation par ***regul()******* avec tous les paramètres choisis:

&gt; rel.reg &lt;- regul(releve$Day, releve\[3:8\], xmin=9, n=63,
+ deltat=21, tol=1.05, methods=c("s","c","l","a","s","a"),
+ window=21)
&gt; rel.reg

Regulation of, by "method" :
 Astegla Chae Dity Gymn Melosul Navi
"spline" "constant" "linear" "area" "spline" "area"
Arguments for "methods" :
tol.type tol rule f periodic window split
"both" "1.05" "1" "0" "FALSE" "21" "100"
44 interpolated values on 63 ( 0 NAs padded at ends )
Time scale :
 start deltat frequency
9.00000000 21.00000000 0.04761905
Time units : days
call : regul(x = releve$Time, y = releve\[3:8\], xmin = 9, n = 63, deltat = 21, tol = 1.05, methods = c("s", "c", "l", "a", "s", "a"), window = 21)

Ainsi, 44 valeurs sont interpolées pour chaque série à l’aide de sa méthode respective, mais cela nous le savions déjà puisque les paramètres de temps pour les séries régulières ont été déterminés précédemment afin que 19 valeurs coïncident dans la fenêtre de tolérance choisie. De même, aucune valeur n’a été extrapolée (aucun **NA** n’a dû être ajouté si **rule = 1** comme ici) puisque **n** a été choisi judicieusement pour ne pas déborder de la série initiale. Il est possible d’avoir également la liste des points interpolés et leur valeur pour chaque série à l’aide de summary(rel.reg). La représentation graphique de n’importe laquelle de ces séries, superposée à la série de départ correspondante est simplement obtenue par ***plot()******* en précisant le numéro de la série à représenter:

&gt; plot(rel.reg, 5)

Les points interpolés sont représentés par une croix et les points qui coïncident à la série initiale (dans la fenêtre de tolérance) sont indiqués par une croix entourée d’un cercle. Les limites inférieures et supérieures des deux séries sont indiquées par des traits pointillés verticaux de couleur correspondante.

Il y a encore deux critères que nous n’avons pas considérés ici: (1) les autres séries (il faut qu’une base de temps commune soit apte à représenter correctement TOUTES les séries, pas seulement une seule), et (2) des contraintes sur le pas de temps d’un point de vue pratique (par exemple, on veut conserver un pas de temps bi-mensuel ou mensuel au lieu de 17 ou 21 jours parce qu’on cherche à mettre en évidence des fluctuations saisonnières et que l’on ne veut pas de décalages du pas de temps d’une année à l’autre).

En particulier, nous n’avons pas respecté la dernière contrainte. Avec une série pluriannuelle comme celle-ci, nous voudrions certainement étudier d’éventuelles variations saisonnières sur la série régularisée. Or, beaucoup de fonctions de R utilisables dans ce but nécessitent que l’unité de temps corresponde à la durée du cycle, c'est-à-dire, **"years"** ici. Bien entendu, il faut au moins 3 ou 4 mesures par unité de temps pour pouvoir étudier le cycle. R impose aussi que l’intervalle de temps soit un multiple exact de l’unité. Nous avons vu plus haut que, à plus ou moins un jour près, nous pouvons considérer qu’une année dure 365.25 jours. Toujours à un ou deux jours près dans la coupure à gauche et/ou à droite des mois, on peut considérer qu’un mois est équivalent à 1/12 d’une année, soit 365.25/12 = 30.44 jours. Si l’on veut s’en tenir à des valeurs bimensuelles ou "quinzaines", on doit donc considérer des intervalles **deltat** de 15.22 jours. L’utilisation de **tol** supérieur ou égale à 1 jour prend ici tout son sens! Ainsi, au lieu de 17 jours, nous serions contraints de prendre 15.22 jours comme intervalle pour pouvoir transposer le temps en **"years"**, avec une fréquence entière **frequency = 24** (24 "quinzaines" par an). Nous n’avons donc plus qu’un paramètre temporel libre pour ajuster au mieux la série régularisée afin d’interpoler le moins de points possibles. Les fonctions ***regul.screen()******* et ***regul.adj()******* nous renseignent toujours sur la valeur optimale de **xmin** dans ce cas précis (les résultats renvoyés ne sont pas imprimés):

&gt; regul.screen(releve$Day, weight, xmin=-1:14, deltat=365.25/24,
+ tol=2.2)
…
&gt; regul.adj(releve$Day, xmin=6, deltat=365.25/24)
…

Nous retiendrons ainsi: **xmin = 6**, **n = 87** et **tol  2.2**. Avec ces valeurs, 24 observations sur les 87 dans la série régulière ne devront pas être interpolées, soit environ 27%. Nous préférons préciser **frequency = 24** à **deltat** (qui serait une fraction puisqu’il vaut 1/24  0.0416667). La régularisation de la série ***Melosul***, avec une fréquence de 24 mesures par an exactement est donc obtenue par:

&gt; melo.regy &lt;- regul(releve$Day, releve$Melosul, xmin=6, n=87,
+ units="daystoyears", frequency=24, tol=2.2, methods="linear",
+ datemin="21/03/1989", dateformat="d/m/Y")

A 'tol' of 2.2 in 'days' is 0.00602327173169062 in 'years'
'tol' was adjusted to 0.00595238095238083

Le programme nous avertit que, puisque nous avons transformé l'échelle temporelle de **"days"** en **"years"** (argument **units="daystoyears"**), la valeur de **tol=2.2** qui était exprimée en jours devient 0.0602… lorsqu'elle est exprimée dans l'unité **"years"**. D'autre part, **tol** doit aussi être une fraction entière de 1/**frequency** (c'est-à-dire de **deltat**, voir ?regul). Or ce n'est pas le cas actuellement. La seconde ligne nous prévient donc que la valeur de **tol** a dû être ajustée à 0.00595 ans, soit 1/168 d'année, soit encore 1/168\*365.25 jours, c'est-à-dire 2.174 jours. Nous savions que **tol** allait être ajustée, puisque nous n’avons fourni qu’une valeur indicative. On précisera **datemin = "21/03/1989"** ou **datemin = releve$Date\[1\]** afin de caler l’axe temporel correctement. Nous obtenons le résultat suivant:

&gt; melo.regy
Regulation of, by "method" :
 Series
"linear"
Arguments for "methods" :
tol.type tol rule f periodic window split
"both" "0.00595” "1" "0" "FALSE" "15.3953488372093" "100"
63 interpolated values on 87 ( 0 NAs padded at ends )
Time scale :
 start deltat frequency
2.109000e+03 4.166667e-02 24
Time units : years
call : regul(x = releve$Time, y = releve$Melosul, xmin = 6, n = 87, units = "daystoyears", frequency = 24, datemin = releve$Date\[1\], tol = 2.2, methods = "linear")

&gt; plot(melo.regy, main="Regulation of Melosul")

On fait un tout petit peu moins bien qu’avec notre pas de temps de 17 jours dans la "capture" des pics et des creux, mais cette fois-ci, on obtient une échelle temporelle utilisable pour étudier les variations saisonnières. En fin de compte, la régularisation retenue sera un compromis entre les meilleures représentations pour les différentes séries et les contraintes imposées. Une fois que l’on est satisfait du résultat obtenu, il est extrêmement facile de convertir l’objet *‘regul’* issu de la régularisation en ‘time series’ régulière(s), c’est à dire, soit en objet *‘rts’* sous S+, soit en objet *‘ts’* sous R à l'aide de la fonction ***tseries()*******:

&gt; melo.ts &lt;- tseries(melo.regy)
&gt; is.tseries(melo.ts)
\[1\] TRUE

On peut aussi décider de n’extraire qu’une seule ou quelques séries régularisées contenues dans un objet *‘regul’* qui en contient plusieurs à l’aide de la méthode ***extract()*******:

&gt; melo.ts2 &lt;- extract(rel.reg, series="Melosul")

A partir d’ici, nous disposons de tous les outils de traitement de séries spatio-temporelles de R, ainsi que des routines spécialisées de PASTECS pour continuer l’analyse: analyse spectrale, lissage, filtrage, décomposition, tendance générale ou saisonnière, etc. (voir chapitres suivants).

> Les quatre fonctions suivantes (**regconst()**, **reglin()**, **regspline()** et **regarea()**) sont utilisées par **regul()**, mais sont aussi fournies séparément pour permettre à l’utilisateur avancé de réaliser ses propres routines de régularisation indépendamment de la fonction **regul()** et de l’objet ‘regul’ correspondant.

### Régularisation par valeur constante

Il s’agit de la méthode la plus simple de régularisation (mais aussi la moins puissante). Les valeurs interpolées *X*<sub>*j*</sub> aux temps *T*<sub>*j*</sub>* *sont calculées comme suit, si les valeurs mesurées qui encadrent *X*<sub>*j*</sub> dans la série irrégulière initiale sont *x*<sub>*i*-1</sub> et *x*<sub>*i*</sub> aux temps *t*<sub>*i*-1</sub> et *t*<sub>*i*</sub> (*i* est choisi tel que *t*<sub>*i*-1</sub> &lt; *T*<sub>*j*</sub>  *t*<sub>*i*</sub>).

avec *f* une constante comprise entre 0 et 1 et qui donne plus de poids à la valeur mesurée à gauche (*f* &lt; 0.5), ou à droite (*f* &gt; 0.5) de la valeur à interpoler. Avec *f* = 1 on a une fonction d’interpolation continue à gauche, avec *f* = 0 on a une fonction continue à droite et avec *f* = 0.5 on obtient la moyenne des deux valeurs encadrantes comme interpolation sur tout l’intervalle. Cette méthode est surtout utile avec *f* = 0, si la série de départ a été compressée avec un algorithme de type *RLE* ("Run-Length Encoding") qui reprend chaque valeur et l’intervalle sur lequel elle reste constante, au lieu de considérer un pas de temps constant.

<span id="anchor-5"></span>Exemple:

La série ***Melosul***** **du jeu de données ***releve*** est régularisée avec la méthode de valeur constante comme suit:

&gt; data(releve)
&gt; reg &lt;- regconst(releve$Day, releve$Melosul)

On peut tracer un graphique de la série de départ et de la série régularisée pour comparaison visuelle:

&gt; plot(releve$Day, releve$Melosul, type="l")
&gt; lines(reg$x, reg$y, col = 2)

La série de départ apparaît en noir, et la série régularisée en rouge. Lorsqu’une interpolation est nécessaire, la valeur immédiatement à gauche dans la série de départ est utilisée. A noter toutefois que cette méthode est incluse dans ***regul()*** qui offre plus de facilités pour le diagnostic et l’extraction des séries temporelles après régularisation. On peut visualiser les points de la série de départ superposés à la fonction continue calculée qui sert à la régularisation comme suit:

&gt; plot(releve$Day, releve$Melosul, type="p")
&gt; lines(regconst(releve$Day, releve$Melosul,
+ n=length(releve$Day)\*10), col=2, lty=2)

### Régularisation linéaire

Les points mesurés sont reliés par des segments de droite, et les valeurs interpolées sont obtenues à partir de la ligne brisée ainsi construite. L’équation qui donne les valeurs interpolées *X*<sub>*j*</sub> au temps *T*<sub>*j*</sub> à partir des valeurs encadrantes *x*<sub>*i*-1</sub>, *x*<sub>*i*</sub> aux temps respectifs *t*<sub>*i*-1</sub>, *t*<sub>*i*</sub> de la série irrégulière initiale est:

avec, pour tout *j* = 1…*p*, *i* est choisi tel que *t*<sub>*i*-1</sub> &lt; *T*<sub>*j*</sub>  *t*<sub>*i*</sub>.

<span id="anchor-6"></span>Exemple:

La série ***Melosul***** **du jeu de données** *****releve***** **est régularisée avec la méthode linéaire comme suit:

&gt; data(releve)\# Dans R uniquement
&gt; reg &lt;- reglin(releve$Day, releve$Melosul)

On peut tracer un graphique de la série de départ et de la série régularisée pour comparaison visuelle:

&gt; plot(releve$Day, releve$Melosul, type="l")
&gt; lines(reg$x, reg$y, col=4)

La série de départ apparaît en noir, et la série régularisée en bleu. L’interpolation est réalisée le long de la droite qui relie les deux valeurs encadrantes. Cette méthode est incluse dans ***regul()*** qui offre plus de facilités pour le diagnostic et l’extraction des séries temporelles après régularisation. On peut visualiser les points de la série de départ superposés à la fonction continue calculée qui sert à la régularisation comme suit:

&gt; plot(releve$Day, releve$Melosul, type="p")
&gt; lines(reglin(releve$Day, releve$Melosul,
+ n=length(releve$Day)\*10), col=4, lty=2)

### Régularisation par courbes splines

L’ajustement par des fonctions splines consiste à définir un polynôme dont les valeurs aux points d’observation coïncident avec les valeurs de l’échantillon. Le problème d’interpolation entre *p* points, donnés par une fonction *g*(*x*) dont les dérivées d’ordre 1 à *k* (avec 1  *k*) sont continues, peut être résolu en minimisant des fonctions du type:

avec: *a* &lt; *x*<sub>1</sub> &lt; … &lt; *x*<sub>*p*</sub> &lt; *b*

Plus on est exigeant sur l’ordre *k* des dérivées que l’on souhaite continues ainsi que sur le degré du polynôme *g*(*x*) choisi et plus l’expression de la fonction spline devient complexe. Nous considérerons des fonctions splines cubiques, c’est-à-dire constituées de polynômes du troisième degré dont les deux premières dérivées sont continues.

Soit *n* valeurs temporelles successives *t*<sub>1</sub>, *t*<sub>2</sub>, …, *t*<sub>*n*</sub> ainsi que les observations correspondantes *x*<sub>1</sub>, *x*<sub>2</sub>, …, *x*<sub>*n*</sub> de la série irrégulière initiale. L’algorithme de calcul est le suivant:

-   On calcule les écarts entre les abscisses temporelles: *H*<sub>*i*</sub> = *t*<sub>*i*</sub> – *t*<sub>*i*-1.</sub>
-   On construit un vecteur colonne des différences *V * tel que:

et une matrice carrée *M* telle que:

-   On calcule les dérivées secondes aux points expérimentaux. Pour cela il faut résoudre le système *M* . *DS* = *V* ou encore *DS* = *M*<sup>-1</sup> . *V*. On obtient ainsi dans le vecteur *DS* les dérivées secondes aux points d’origine. On attribue des dérivées secondes nulles aux points extrêmes de la courbe (argument **method = "fmm"** de ***regspline()***), ou on considère que la courbe forme une boucle (dernière valeur = première valeur, argument **method = "periodic"**).
-   On calcule la fonction spline pour chaque valeur de temps *T*<sub>*j*</sub> de la série régularisée selon un pas préalablement choisi. En appelant *X*<sub>*j*</sub>, les ordonnées calculées de la série régularisée correspondant à ces temps *T*<sub>*j*</sub>, on a:

avec *i* choisi tel que *t*<sub>*i-1*\\ </sub> &lt; *T*<sub>*j*</sub>* *  *t*<sub>*i*</sub> pour chaque valeur de *j *= 1…*p*.

> Etant donné que les courbes splines peuvent dans certains cas produire des valeurs plus grandes ou plus petites que les valeurs extrêmes de la série initiale, la méthode utilisée dans PASTECS recadre ces extrémas pour éviter qu’ils ne soient plus grands ou plus petits que les valeurs extrêmes de la série de départ. Cela évite, par exemple, d’obtenir des valeurs négatives pour des comptages dans certains cas.

##### Exemple {-}

La série ***Melosul***** **du jeu de données ***releve*** est régularisée avec des splines comme suit:

&gt; data(releve)\# Dans R uniquement
&gt; reg &lt;- regspline(releve$Day, releve$Melosul)

On peut tracer un graphique de la série de départ et de la série régularisée pour comparaison visuelle:

&gt; plot(releve$Day, releve$Melosul, type="l")
&gt; lines(reg$x, reg$y, col=3)

La série de départ apparaît en noir, et la série régularisée en vert. Les valeurs interpolées le sont le long de segments de courbes appelés splines. A noter toutefois que cette méthode est incluse dans ***regul()*** qui offre plus de facilités pour le diagnostic et l’extraction des séries temporelles après régularisation. On peut visualiser les points de la série de départ superposés à la fonction continue calculée qui sert à la régularisation comme suit:

&gt; plot(releve$Day, releve$Melosul, type="p")
&gt; lines(regspline(releve$Day, releve$Melosul,
+ n=length(releve$Day)\*10), col=3, lty=2)

### Régularisation par la méthode des aires

Une autre méthode d’interpolation linéaire correspond à l'interpolation par les aires décrites par Fox (1972). Cette méthode permet d'estimer les valeurs manquantes, mais plus généralement de substituer à une série irrégulière, des valeurs régulièrement espacées dans le temps en prenant en compte toutes les valeurs mesurées dans un intervalle constant à gauche et à droite de la valeur interpolée. Contrairement aux autres méthodes de régularisation, la courbe obtenue ne passe pas nécessairement par les points observés intiaux, c’est-à-dire qu’un lissage est effectué simultanément à l’interpolation. Ce lissage est d’autant plus important que la fenêtre d’interpolation est grande.

Soit une série d’observations *x*<sub>1</sub>, *x*<sub>2</sub>, ..., *x*<sub>*n*</sub> aux temps *t*<sub>1</sub>, *t*<sub>2</sub>,..., *t*<sub>*n*</sub>. On considère que chaque valeur *x*<sub>*i*</sub> représente une estimation des valeurs comprises dans l'intervalle de longueur , c’est-à-dire que l’on considère en fait des **aires** successives rectangulaires, plutôt que des observations ponctuelles discontinues dans le temps.

Un intervalle de temps *U* (une fenêtre), est choisi de préférence plus grand que la moyenne des durées minimums séparant les observations successives sur l'ensemble des séries initiales traitées. Considérons des fenêtres *U*<sub>*j*</sub> centrées autour des valeurs *T*<sub>*j*</sub> choisies pour la série régulière calculée. Ces fenêtres *U*<sub>*j*</sub> sont limitées par les dates *U*<sub>-*j*</sub> = *T*<sub>*j*</sub> – *U*/2 et *U*<sub>+*j*</sub> = *T*<sub>*j*</sub> + *U*/2. Le cas le plus simple correspond à la coïncidence entre le point au centre de la fenêtre et la date d'observation. Dans ce cas il n'est pas effectué de régularisation et la valeur observée reste inchangée (on peut aussi décider d’effectuer le calcul sur ce point à l’aide de l’option `interp = TRUE`, voir description de la fonction ***regarea()***). Dans les autres cas, l'interpolation prend en compte tous les intervalles *f*<sub>*i*</sub> inclus dans la fenêtre. Soit la fraction de *F*<sub>*i*</sub> appartenant à la fenêtre *U*<sub>*j*</sub>. Pour chaque observation, trois cas peuvent apparaître:

1.  les observations telles que (aire complètement inclue dans la fenêtre),
2.  les observations telles que (aire totalement en dehors de la fenêtre),
3.  les observations telles que (intervalle coupant un bord de la fenêtre).

On a: . La valeur interpolée est donnée par .

La figure illustre ce procédé. Il faut estimer la valeur *X*<sub>*j*</sub> au temps *T*<sub>*j*</sub>. Les points *x*<sub>*i*-2</sub>, *x*<sub>*i*-1</sub> et *x*<sub>*i*</sub> sont inclus dans la fenêtre. Les points *x*<sub>*i*-3</sub> et *x*<sub>*i*+1</sub> sont les proches à l'extérieur. Les valeurs réelles observées sont: *x*<sub>*i*-3</sub> = 10, *x*<sub>*i*-2</sub> = 5, *x*<sub>*i*-1</sub> = 9, *x*<sub>*i*</sub> = 3, *x*<sub>*i*+1</sub> = 6, et les correspondants: 0, 4, 6, 7, 1.

On a: .

D'où: *X*<sub>*j*</sub> = (10 . 0 + 5 . 4 + 9 . 6 + 3 . 7 + 6 . 1) / 18 = 5,6

Notons qu'une valeur nulle interpolée correspond soit:

-   au cas où toutes les observations *x*<sub>*i*</sub> satisfaisant sont égales à 0,
-   à la coïncidence de dates entre un 0 observé et le milieu de la fenêtre: *T*<sub>*j*</sub> = *t*<sub>*i*</sub> avec *x*<sub>*i*</sub> = 0 lorsque les données qui coïncident ne sont pas interpolées (argument `interp = FALSE` de ***regarea()***).

L’algorithme utilisé dans l'ancien PASSTEC 2000 correspond à peu de choses près à cette description. Le code se caractérise par des boucles (sur toutes les valeurs extrapolées) et de nombreux tests de conditions. L’algorithme a dû donc être totalement réécrit en calcul matriciel pour la librairie PASTECS. Quatre vecteurs sont créés: *t*<sub>-</sub>, *t*<sub>+</sub> et *U*<sub>-</sub>, *U*<sub>+</sub>. Les vecteurs *t*<sub>-</sub> et *t*<sub>+</sub> contiennent respectivement les bornes inférieures et supérieures de chaque aire correspondant aux points *x*<sub>*i*</sub> mesurés. Leur dimension est donc *n*, le nombre de points dans la série initiale. De même, les vecteurs *U*<sub>-</sub> et *U*<sub>+</sub> contiennent les bornes inférieures et supérieures des fenêtres entourant chaque point *X*<sub>*j*</sub> à extrapoler au temps *T*<sub>*j*</sub>. Leur dimension est *p*, le nombre de points à obtenir dans la série régularisée. Les vecteurs *t*<sub>-</sub> et *t*<sub>+</sub> sont recopiés *p* fois en colonnes, de manière à former deux matrices *n*  *p* (lignes  colonnes). De même, les vecteurs *U*<sub>-</sub> et *U*<sub>+</sub> sont recopiés *n* fois en lignes pour former également deux matrices de dimension *n * *p*. On peut montrer que les pour toutes les valeurs à extrapoler sont obtenus dans les colonnes respectives de *F* calculé comme suit:

Et ce, à condition que:

-   toutes les valeurs contenues dans les vecteurs *t*<sub>-</sub>, *t*<sub>+</sub>, *U*<sub>-</sub>, *U*<sub>+</sub> soient positives (il suffit de soustraire la plus petite valeur de temps rencontrée à chaque vecteur, si cette valeur est inférieures à 0, pour obtenir cette condition),
-   toutes les valeurs négatives obtenues pour les *F*<sub>*ij*</sub> soient remplacées par des zéros (elles correspondent à des plages totalement en dehors de la fenêtre *U*<sub>*j*</sub>).

Par conséquent, les valeurs interpolées sont obtenues dans le vecteur *X*<sub>*j*</sub> par:

où *i* est l’indice des lignes et *j* est l’indice des colonnes pour la matrice *F* (somme par colonne du produit *x*<sub>*i*</sub>.*F*<sub>*ij*</sub> divisé par la taille de fenêtre *U*).

L’inconvénient de cette méthode est de produire 4 matrices de très grandes tailles *n*  *p* pour des séries longues. De plus, comme la plupart des *F*<sub>*ij*</sub> sont nuls (en dehors de *U*) pour de très longues séries de taille nettement supérieure à la taille de la fenêtre *U*, cette méthode utilise une grande quantité de mémoire vive et de calcul dans pareil cas. La solution est de diviser la série en sous-unités plus petites et d’effectuer l’extrapolation sur chaque sous-unité indépendamment le long de la "diagonale" *j*  *i*. Nous avons déterminé empiriquement que, sous S+ comme sous R, la taille de sous-unité qui optimise la vitesse de calcul est d’environ 100 valeurs interpolées calculées à la fois. Cette taille de sous-unité est toutefois ajustable par l’argument **split** de la fonction ***regarea()***. Si le nombre de valeurs dans la série initiale est sensiblement égal au nombre de valeurs extrapolées, les matrices ont une taille d’environ 100  100, soit 10000 éléments. Notre machine de référence (Pentium II 500 Mhz avec 128 Mo de mémoire vive) est capable de régulariser la série ***Melosul*** de la matrice exemple ***releve*** (200 valeurs extrapolées, et fenêtre de 50 jours) en 0.175 sec avec `split = 100`, contre 2.68 sec avec `split = 1` (correspondant à peu près à la méthode de l'ancien PASSTEC 2000 de calcul valeur par valeur).

##### Exemple {-}

La série ***Melosul***** **du jeu de données ***releve*** est régularisée avec la méthode des aires, en utilisant une fenêtre de 25, puis de 50 jours) comme suit:

&gt; data(releve)
&gt; reg &lt;- regarea(releve$Day, releve$Melosul, window=25)
&gt; reg2 &lt;- regarea(releve$Day, releve$Melosul, window=50)

On peut tracer un graphique de la série de départ et des deux séries régularisées pour comparaison visuelle:

&gt; plot(releve$Day, releve$Melosul, type="l")
&gt; lines(reg$x, reg$y, col=2)
&gt; lines(reg2$x, reg2$y, col=4)

La série de départ apparaît en noir, la série régularisée avec une fenêtre de 25 jours est tracée en rouge, et celle régularisée avec une fenêtre de 50 jours, en bleu. Lorsqu’une interpolation est nécessaire, une moyenne des aires incluses dans la fenêtre est réalisée. Plus cette fenêtre est large, plus les valeurs interpolées seront "lissée" par rapport à la série de départ. A noter toutefois que cette méthode est incluse dans ***regul()*** qui offre plus de facilités pour le diagnostic et l’extraction des séries temporelles après régularisation. Les deux graphiques suivants montrent les fonctions continues calculées qui servent à la régularisation de l'exemple précédent pour respectivement un fenêtre de 25 jours (en rouge) et de 50 jours (en bleu):

&gt; plot(releve$Day, releve$Melosul, type="p")
&gt; lines(regarea(releve$Day, releve$Melosul, window=25,
+ n=length(releve$Day)\*10), col=2, lty=2)
&gt; plot(releve$Day, releve$Melosul, type="p")
&gt; lines(regarea(releve$Day, releve$Melosul, window=50,
+ n=length(releve$Day)\*10), col=4, lty=2)

### Conversion en séries régulières

Comme nous l'avons vu au paragraphe présentant les fonctions générales de régularisation, la fonction ***tseries()******* permet de convertir un objet *‘regul’* issu d’une régularisation (de même d’ailleurs qu’un objet *‘tsd’* issu d’une décomposition, voir plus loin) en objets *‘ts’* contenant **toutes** les séries. Pour extraire **une partie** des séries présentes dans l’objet *‘regul’*, utiliser plutôt la méthode ***extract()******* de l’objet. Enfin, la fonction ***is.tseries()******* est utile pour tester si un objet est une série régulière. Elle est équivalent à la fonction ***is.ts()******* proposées en standard dans R.
