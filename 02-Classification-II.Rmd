# Classification supervisée II {#classif2}

##### Objectifs {-}

- Maîtriser la validation croisée comme approche efficace de partitionnement entre set d'apprentissage et de test

- Connaître la classification supervisée par la méthode des plus proches voisins

- Connaître également sa variante optimisée\ : la quantification vectorielle

##### Prérequis {-}

Ce module continue la matière vue dans le module précédent et s'appuie dessus. Assurez-vous d'avoir bien compris la logique de la classification supervisée et les trois étapes\ : apprentissage, test et déploiement. Vous devez être également capable de créer un classifieur avec R (en utilisant l'ADL). Enfin, la matrice de confusion et les métriques qui en sont dérivées ne doivent plus avoir de secrets pour vous pour évaluer les performances d'un classifieur dans la phase de test. S'il subsiste des doutes ou des incompréhensions pour ces aspects clés de la classification supervisée, vous feriez mieux d'approfondir le module 1 *avant* d'aborder celui-ci.

## Validation croisée

Rappelez-vous qu'une règle à laquelle il ne faut *jamais* déroger, c'est de ne pas utiliser les mêmes individus en apprentissage et en test.

![Les sets d'apprentissage et de test ne peuvent pas utiliser *toutes* les données\ : il faut les partitionner.](images/02-classification2/split-set.png)

Souvent, la grosse difficulté est d'obtenir suffisamment d'objets de chaque classe identifiés manuellement pour permettre à la fois l'apprentissage et le test. Un test sur les **mêmes** objets que ceux utilisés lors de l'apprentissage mène à une surestimation systématique des performances du classifieur. Nous sommes donc contraint d'utiliser des objets différents dans les deux cas. Naturellement, plus, nous avons d'objets dans le set d'apprentissage **et ** dans le set de test, et meilleur sera notre classifieur et notre évaluation de son efficacité.

La **validation croisée** permet de résoudre ce dilemme en utilisant en fin de compte *tous* les objets, *à la fois* dans le set d'apprentissage et dans le set de test, mais *jamais simultanément*. L'astuce consiste à diviser aléatoirement l'échantillon en *n* sous-ensembles à peu près égaux en nombre. Ensuite, l'apprentissage suivi du test est effectué *n* fois indépendamment. A chaque fois, on sélectionne tous les sous-ensembles sauf un pour l'apprentissage, et le sous-ensemble laissé de côté est utilisé pour le test. L'opération est répétée de façon à ce que chaque sous-ensemble serve de set de test tour à tour. Au final, ou rassemble les résultats obtenus sur les *n* sets de tests, donc, sur tous les objets de notre échantillon. Cela se fait en additionnant les *n* matrices de confusion ainsi obtenues. L'animation suivante visualise le processus pour que cela soit plus clair dans vottre esprit.

TODO: ajouter l'animation.

Au final, nous aurons utilisé tous les individus à la fois en apprentissage et en test, mais jamais simultanément. L'addition des matrices de confusions obtenues à chaque étape nous donne une grosse matrice qui contient le même nombre d'individus que l'ensemble de notre jeu de données initial. Naturellement, nous n'avons pas le même classifieur à chaque étape, et celui-ci n'est pas aussi bien construit que s'il utilisait véritablement *toutes* les observations. Mais plus le découpage est fin et plus nous nous en approchons. À la limite, pour *N* observations, nous pourrions réaliser *n* = *N* sous-ensembles, c'est-à-dire que chaque sous-ensemble contient un et un seul individu. Nous avons alors à chaque fois le classifieur le plus proche possible de celui que l'on obtiendrait avec véritablement toutes les observations puisqu'à chaque étape nous ne perdons qu'un seul individu en phase d'apprentissage. La contrepartie est un temps de calcul potentiellement très, très long puisqu'il y a énormément d'étapes. **Cette technique porte le nom de validation par exclusion d'une donnée ou _leave-one-out cross-validation_ en anglais, LOOCV en abbrégé**. À l'autre extrême, nous pourrions utiliser *n* = 2. Mais dans ce cas, nous n'utilisons que le moitié des observation en phase d'apprentissage à chaque étape. C'est le plus rapide, mais le moins exact.

En pratique, un compromis entre exactitude et temps de calcul nous mène à choisir souvent la **validation croisée dix fois** (*ten-fold cross-validation* en anglais). Nous divisons aléatoirement en dix sous-ensembles d'à peu près le même nombre d'individus et nous répétons donc l'opération apprentissage -> test seulement dix fois. Chacun des dix classifieurs a donc été élaboré avec 90% des données totales, ce qui représente souvent un compromis encore acceptable pour estimer les propriétés qu'aurait le classifieur réalisé avec 100% des données. Toutefois, si nous constatons que le temps de calcul est raisonnable, rien ne nous empêche d'augmenter le nombre de sous-ensemble, voire d'utiliser la version par exclusion d'une donnée, mais en pratique nous observons tout de même que cela n'est pas raisonnable sur des très gros jeux de données et avec les algorithmes les plus puissants, mais aussi les plus gourmands en calcul comme random forest ou les réseaux de neurones que nous aborderons au prochain chapitre.

Appliquons cela tout de suite avec l'ADL sur nos manchots. Plus besoin de séparer le jeu de données en set d'apprentissage et de test indépendants. La fonction `cvpredict(, cv.k = ...)` va se charger de ce partitionnement selon l'approche en `cv.k` étapes décrite ci-dessus. Notre analyse devient alors\ :

```{r, message=FALSE, warning=FALSE}
SciViews::R
library(mlearning)

# Importation et remaniement des données comme précédemment
read("penguins", package = "palmerpenguins") %>.%
  rename(., bill_length = bill_length_mm, bill_depth = bill_depth_mm, 
    flipper_length = flipper_length_mm, body_mass = body_mass_g) %>.%
  select(., -year, -island, -sex) %>.%
  drop_na(.) -> penguins
```

Une fois notre tableau complet correctement nettoyé et préparé, nous faisons\ :

```{r}
# ADL avec toutes les données
penguins_lda <- mlLda(data = penguins, species ~ .)
# Prédiction par validation croisée 10x
set.seed(7567)
penguins_pred <- cvpredict(penguins_lda, cv.k = 10)
# Matrice de confusion
penguins_conf <- confusion(penguins_pred, penguins$species)
plot(penguins_conf)
```

Ici, nous avons cinq erreurs, mais attention, ceci est comptabilisé sur trois fois plus de données que précédemment, puisque l'ensemble du jeu de données a servi ici en test, contre un tiers seulement auparavant. Donc, notre estimation des performances du classifieur est assez comparable, avec une parfaite séparation de Gentoo, mais une petite erreur entre Chinstrap et Adelie. Les métriques sont disponibles à partir de notre objet `penguins_conf` comme d'habitude\ : 

```{r}
summary(penguins_conf)
```

Précédemment, nous avions 1,8% d'erreur, et maintenant, nous n'en avons plus que 1,5%. C'est normal que notre taux d'erreur baisse un petit peu car nos classifieurs par validation croisée utilisent 90% des données alors qu'auparavant, nous n'utilisions de les 2/3.

Si nous voulons faire un "leave-one-out", nous ferions (sachant que nos données comptent `r nrow(penguins)` cas, nous indiquons ici `cv.k = 342`)\ :

```{r}
penguins_pred_loo <- cvpredict(penguins_lda, cv.k = 342)
# Matrice de confusion
penguins_conf_loo <- confusion(penguins_pred_loo, penguins$species)
plot(penguins_conf_loo)
```

```{r}
summary(penguins_conf_loo)
```

Le résultat est le même. Donc, nous venons de montrer que, dans le cas de ce jeu de données et de l'ADL, une validation croisée dix fois permet d'estimer les performances du classifieur aussi bien que l'approche bien plus coûteuse en calculs (342 classifieurs sont calculés et testés) du "leave-one-out".

## Différents algorithmes

Les algorithmes de classification supervisée se subdivisent en trois grandes classes\ :

- celles qui **utilisent un modèle** (linéaire ou non linéaire) sous-jacent pour mettre en relation les mesures et les groupes. Par exemple, l'analyse discriminante linéaire que nous connaissons déjà appartient à cette catégorie. Nous verrons également les machines à vecteurs supports et les réseaux de neurones dans cette catégorie.

- celles qui font appel à un indice de similarité calculé entre les individus (attribution du groupe correspondant aux individus les plus ressemblants à celui qu'on teste). Ce sont les techniques dites du plus proche voisin et de la quantification vectorielle (voir ci-après dans ce module).

- enfin, les techniques qui définissent une suite de règle de division dichotomique du jeu de données (qui se matérialisent par un ou plusieurs **arbres de décision**). Il s'agit du partitionnement récursif et de "random forest", par exemple. Nous verrons ces méthodes dans le module 3.

Nous avons déjà abordé l"analyse discriminante linéaire dans le précédent module et nous allons étudier ici quelques autres algorithmes de chaque groupe ici, ainsi que dans le module suivant. Ensuite, nous découvrirons une méthode efficace de partitionnement entre set d'apprentissage et de test\ : la validation croisée. Enfin, nous mettrons tout cela en musique pour étudier différents algorithmes de classification en pratique, en vue de choisir celui qui nous semble le plus adéquat pour le cas étudié.

### Indiens diabétiques

Afin d'explorer et comparer l'utilisation de différents algorithmes de classification supervisée, nous reprendrons notre jeu de données `pima` concernant une population d'Amérindiens qui sont connus pour compter un haut taux d'obèses et de diabétiques. Nous avions déjà utilisé ces données pour [illuster l'ACP](https://wp.sciviews.org/sdd-umons2/?iframe=wp.sciviews.org/sdd-umons2-2020/analyse-en-composantes-principales.html). Pour rappel, le jeu de données se présente comme suit\ :

```{r}
SciViews::R
pima <- read("PimaIndiansDiabetes2", package = "mlbench")
pima
```

Nous avons huit variables quantitatives (discrète comme `pregnant`, ou continues pour les autres) et une variable qualitative `diabetes`. Voici quelques informations sur ces différentes variables\ :
- `diabetes`, variable qualitative à deux niveaux indique si l'individu est diabétique (`pos`) ou non (`neg`). C'est naturellement la variable réponse que l'on cherchera à prédire ici,
- `pregnant` est le nombre de grossesses que cette femme a eue (il s'agit uniquement d'un échantillon de femmes),
- `glucose` est le taux de glucose dans le plasma sanguin (test standardisé renvoyant une valeur sans unités),
- `pressure` est la pression sanguine diastolique, en mm de mercure,
- `triceps` est l'épaisseur mesurée d'un pli de peu au niveau du triceps en mm. Il s'agit d'une mesure permettant d'estimer l'obésité, ou en tous cas, la couche de graisse sous-cutanée à ce niveau,
- `insulin` est la détermination de la quantité d'insuline deux heures après prise orale de sucre, en µU/mL,
- `mass` est en réalité l'IMC, indice de masse corporelle que vous connaissez bien (masse/ taille^2^), en kg/m^2^, un autre indice d'obésité couramment employé,
- `pedigree` est un indice de prédisposition au diabète établi en fonction des informations sur la famille (sans unités),
- `age`est l'âge de l'indienne exprimé en années.

En fonction de ces informations, nous pouvons labelliser notre jeu de données comme suit\ :

```{r}
pima <- labelise(pima,
  label = list(
    diabetes = "Diabète",
    pregnant = "Grossesses", glucose = "Test glucose",
    pressure = "Pression sanguine", triceps = "Gras au triceps",
    insulin = "Insuline", mass = "IMC",
    pedigree = "Pedigree", age = "Âge"),
  units = list(
    pressure = "mm Hg", triceps = "mm",
    insulin = "µU/mL", mass = "kg/m^2", age = "années"
  )
)
```

Ce jeu de données contient 768 cas, mais deux variables (`triceps` et `insulin`) ont un très grand nombre de valeurs manquantes.

```{r}
naniar::vis_miss(pima)
```

Comme pour l'ACP, nos sets d'apprentissage et de test ne peuvent pas contenir de valeurs manquantes. Si nous utilisons `drop_na()` sur tout le tableau, toute ligne contenant au moins une valeur manquante sera éliminée. Cela donne ceci\ :

```{r}
pima %>.%
  drop_na(.) -> pima1
nrow(pima1)
```

Nous avons certes un tableau propre, mais nous avons perdu près de la moitié des données\ ! Or nous n'avons jamais assez de données en classification supervisée. Nous pourrions aussi considérer la possibilité de laisser tomber les *colonnes* qui contiennent trop de valeurs manquantes. En première approche, afin de déterminer si la perte de ces variables pourrait être préjudiciable à notre analyse, nous pourrions inspecter la matrice de corrélation.

```{r}
pima1 %>.%
  select(., -diabetes) %>.%
  correlation(.) %>.%
  plot(.)
```

Nous observons que `triceps` est le plus fortement corrélé à `mass`, ce qui est logique puisqu'il s'agit de deux mesures différentes de l'obésité.

```{r}
chart(data = pima1, triceps ~ mass %color=% diabetes) +
  geom_point() +
  stat_smooth(method = "lm")
```

De même, `insulin` est corrélée à `glucose`, également deux tests qui étudient le profil de variation du sucre dans le sang et d'une hormone associée.

```{r}
chart(data = pima1, insulin ~ glucose %color=% diabetes) +
  geom_point() +
  stat_smooth(method = "lm")
```
Cependant, les corrélations de Pearson sont moyenne (0,66 et 0,58, respectivement) et les nualges de points assez dispersés. Nous pourrions donc nous demander s"'il vaut mieux garder plus de données avec moins de variables pour notre apprentissage et test... nous allons créer `pima2` sans `insulin` et `triceps` et nous comparerons l'analyse faite avec `pima1` (plus de variables, moins de cas) et `pima2` (moins de variables, plus de cas).

```{r}
pima %>.%
  select(., -insulin, -triceps) %>.%
  drop_na(.) -> pima2
nrow(pima2)
```

Dans ce second jeu de données nous avons pu tout de même conserver `r nrow(pima2)` cas. [L'ACP que nous avions réalisée l'an dernier sur ces données](https://wp.sciviews.org/sdd-umons2/?iframe=wp.sciviews.org/sdd-umons2-2020/analyse-en-composantes-principales.html) nous montrait que la variance se répartir à 53% sur deux axes, mais qu'il faut considérer 5 axes pour capturer 90% de cette variance. Ceci suggère, comme la matrice de corrélation, que les différentes variables apportent chacune un information complémentaire. Au final, nous n'observions pas de séparation nette sur le graphique des individus de l'ACP entre la sous-population diabétique et celle qui ne l'est pas. Nous allons voir ci quelles sont les possibilités de *prédire* qui est diabétique ou non en fonction des six (`pima1`) ou huit (`pima2`) attributs à disposition, et ce, à l'aide de différent algorithmes de classification supervisée. Commençons par voir ce que cela donnes avec l'ADL déjà étudiée au module précédent.

Ici, nous évaluons les performances à l'aide de la validation croisée dix fois que nous venons d'étudier.

```{r}
library(mlearning)
pima1_lda <- mlLda(data = pima1, diabetes ~ .)
pima1_pred <- cvpredict(pima1_lda, cv.k = 10)
pima1_conf <- confusion(pima1_pred, pima1$diabetes)
plot(pima1_conf)
summary(pima1_conf)
```

Et avec `pima2, cela donne\ :

```{r}
pima2_lda <- mlLda(data = pima2, diabetes ~ .)
pima2_pred <- cvpredict(pima2_lda, cv.k = 10)
pima2_conf <- confusion(pima2_pred, pima2$diabetes)
plot(pima2_conf)
summary(pima2_conf)
```


Nous avons 22% d'erreur avec `pima1` et 23% d'erreur avec `pima2`. Ces résultats se tiennent dans le cas présent. Ce n'est évidemment pas toujours le même résultat. Nous allons voir ce que cela donnez avec d'autres algorithmes de classification.

## K plus proches voisins

K plus proches voisins (*k-nearest neighbours* en anglais ou k-NN en abbrégé) est certainement la technique la plus intuitive en classification supervisée. Malgré sa simplicité inhérente, elle offre de bonnes prestations. La classification supervisée s’effectue par une analyse de la matrice de [distances de Mahalanobis](https://qastack.fr/stats/62092/bottom-to-top-explanation-of-the-mahalanobis-distance) (équivalente à la distance Euclidienne ou géométrique appliquée sur des données réduites de variance unitaire) entre un individu d’intérêt à reconnaître et les individus du set d'apprentissage.

![La classification par les k plus proches voisins compare un individu inconnu (en gris) avec les individus de classe connue du set d'apprentissage (bleus et rouges matérialisant deux classes différentes) du point de vue de la distance de Mahalanobis. Ici k = 3, et l'individu inconnu est considéré comme bleu puisqu'il est plus proche de deux bleus contre un seul rouge.](images/02-classification2/knn.png)

En d’autres termes, il s’agit tout simplement de calculer la distance géométrique qui sépare un individu d’intérêt de tous les individus du set d'apprentissage dans un espace réduit, c’est-à-dire, un espace où chaque variable est mise à l’échelle de telle manière que sa variance soit unitaire. La classe attribuée à l'individu d’intérêt sera la même que celle du, ou des *k* individus les plus proches (d'où le nom de la méthode). Des variantes utilisent naturellement d’autres calculs de distances\ : Euclidiennes, Manhattan, etc.

Un seul paramètre doit donc être définit\ : *k*, représentant le nombre d'individus proches considérés. Un vote à la majorité permet de déterminer à quel groupe appartient l’objet testé. En cas d’ex-æquos, la classe de l'individu du set d'apprentissage le plus proche est utilisée. Pour minimiser le risque d’ex-aequos, *k* est généralement choisi impair. Dans notre schéma, nous utilisons *k* = 3, valeur qui s’est avérée optimale dans beaucoup de situations. La recherche de la valeur optimale de *k* dans le cadre de l’application finale sera évidemment possible ultérieurement.

TODO: exemple...

## Quantification vectorielle

Un des désavantages de la méthode des k plus proches voisins est que l'on doit conserver *toutes* les données du set d'apprentissage et confronter systématiquement tout nouvel individu à l'ensemble des cas de ce set. L'idée sous-jacente à la quantification vectorielle (*learning vector quantization* en anglais, LVQ en abbrégé) est que les données initiales peuvent sans doute être synthétisées\ : des individus proches dans l'espace de Malahonobis peuvent être remplacés par des "individus moyens ou centroïdes" tout en apportant à peu près le même effet. Du coup, nous réduisons la quantité d'information à conserver et nous accélérons les calculs dans les phases de test et déploiement.

Une étape supplémentaire dans le calcul du classifieur est introduite. Le set d'apprentissage initial est résumé en un "codebook" dans lequel les groupes initiaux sont remplacés par un ou plusieurs centroïdes. Ces centroïdes sont, en quelque sorte, des "portraits robots" des différents classes et ils résument les caractéristiques des individus appartenant à ces classes. La classification supervisée se fait par une méthode similaire aux k plus proches voisins mais en utilisant les centroïdes du "codebook" en lieu et place des individus du set d'apprentissage.

En plus du paramètre *k*, il faut donc également définir *nc*, le nombre de centroïdes qui seront calculés dans le "codebook". Il est possible de calculer librement les différents centroïdes en fonction de la dispersion des données, ou bien d’imposer que le même nombre de centroïdes soit calculé pour chaque classe.

![Un exemple fictif et visuel de classification par quantification vectorielle à deux classes (rouge et bleue) résumée via un "codebook" de *nc* = 6. Ce codebook est ensuite utilisé comme référence pour déterminer la classe de l'individu inconnu en gris par k plus proches voisins avec *k* = 3. On en déduit qu'il s'agit probablement d'un individu de la classe rouge ici puisque deux centroïdes rouges sont plus proches contre un seul bleu.](images/02-classification2/lvq.png)

TODO: exemple d'application.



## Choix

TODO: comparaison et choix...
